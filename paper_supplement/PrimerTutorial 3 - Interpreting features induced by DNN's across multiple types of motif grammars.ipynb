{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: \n",
    "* investigate nan's in test set \n",
    "* threshold motif scores at 0 \n",
    "* overlay motif scores and deepLIFT tracks \n",
    "* add grad x input \n",
    "* PRC curves \n",
    "* option of shuffled reference & background frequencies for deepLIFT \n",
    "* fixed axes on deepLIFT plots\n",
    "\n",
    "\n",
    "# How to train your DragoNN tutorial 3: \n",
    "## Interpreting features induced by DNN's across multiple types of motif grammars \n",
    "\n",
    "This tutorial is a supplement to the DragoNN manuscript and follows figure 7 in the manuscript. \n",
    "\n",
    "This tutorial will take 1 hour  if executed on a GPU. \n",
    "\n",
    "Please complete \"Primer Tutorial 1- Exploring model architectures for a homotypic motif density simulation\" prior to completing this tutorial. \n",
    "\n",
    "The architectures used in this tutorial were determined as optimal by hyperparameter grid search in \"Primer Tutorial 3 - CNN Hyperparameter Tuning via Grid Search\"\n",
    "\n",
    "\n",
    "## Outline<a name='outline'>\n",
    "<ol>\n",
    "    <li><a href=#1>How to use this tutorial</a></li>\n",
    "    <li><a href=#2>Defining helper functions for model training and interpretation</a></li>\n",
    "    TODO: explain reference options. \n",
    "    <li><a href=#3>Simulating training data with simdna: Review of Tutorial 1</a></li>\n",
    "    <li><a href=#4>Single Motif</a></li>\n",
    "    <li><a href=#5>Homotypic motif density detection</a></li>\n",
    "    <li><a href=#6>Homotypic motif density localization</a></li>\n",
    "    <li><a href=#7>Multiple motifs (multi-task)</a></li>  \n",
    "    <li><a href=#8>Heterotypic motifs spatial grammar</a></li>\n",
    "    <li><a href=#9>Conclusions</a></li>\n",
    "</ol>\n",
    "Github issues on the dragonn repository with feedback, questions, and discussion are always welcome.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use this tutorial<a name='1'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "This tutorial utilizes a Jupyter/IPython Notebook - an interactive computational enviroment that combines live code, visualizations, and explanatory text. The notebook is organized into a series of cells. You can run the next cell by cliking the play button:\n",
    "![play button](./primer_tutorial_images/play_button.png)\n",
    "You can also run all cells in a series by clicking \"run all\" in the Cell drop-down menu:\n",
    "![play all button](./primer_tutorial_images/play_all_button.png)\n",
    "Half of the cells in this tutorial contain code, the other half contain visualizations and explanatory text. Code, visualizations, and text in cells can be modified - you are encouraged to modify the code as you advance through the tutorial. You can inspect the implementation of a function used in a cell by following these steps:\n",
    "![inspecting code](./primer_tutorial_images/inspecting_code.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the lines below if you are running this tutorial from Google Colab \n",
    "#!pip install https://github.com/kundajelab/simdna/archive/0.3.zip\n",
    "#!pip install https://github.com/kundajelab/dragonn/archive/keras_2.2_tensorflow_1.6_purekeras.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#To prepare for model training, we import the necessary functions and submodules from keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta, SGD, RMSprop;\n",
    "import keras.losses;\n",
    "from keras.constraints import maxnorm;\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping, History\n",
    "from keras import backend as K \n",
    "K.set_image_data_format('channels_last')\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading dragonn's tutorial utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/annashch/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vmin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d98a7f23c2f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdragonn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorial_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dragonn/dragonn/tutorial_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mplot_ism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mism_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;31m# create discrete colormap of ISM scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mextent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mism_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mism_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vmin' is not defined"
     ]
    }
   ],
   "source": [
    "#load dragonn tutorial utilities \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dragonn.tutorial_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining helper functions for model training and interpretation  <a name='2'>\n",
    "<a href=#outline>Home</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each type of simulation, we will perform a consistent set of tasks: \n",
    "* Define the optimal model architecture, as determined in Tutorial 2. This architecture will be specific to the simulation used, so we don't write a universal helper function for this purpose. \n",
    "* Train the model on simulation data and visualize the model's learning curve on training and validation data. \n",
    "* Compute the model's performance on a held-out test set.\n",
    "* Visualize motif scores for a positive and negative example. \n",
    "* Perform in silico mutagenesis for a positive and negative example.\n",
    "* Compute DeepLIFT scores for a positive and negative example.\n",
    "\n",
    "To avoid writing the same code for each scenario, we define a series of helpers functions to perform the tasks above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.callbacks import * \n",
    "from dragonn.tutorial_utils import plot_learning_curve\n",
    "\n",
    "def train_model(model,data):\n",
    "    #We define a custom callback to print training and validation metrics while training. \n",
    "    metrics_callback=MetricsCallback(train_data=(data.X_train,data.y_train),validation_data=(data.X_valid,data.y_valid))\n",
    "    \n",
    "    #Train the model \n",
    "    history=model.fit(x=data.X_train,\n",
    "                                  y=data.y_train,\n",
    "                                  batch_size=128,\n",
    "                                  epochs=150,\n",
    "                                  verbose=0,\n",
    "                                  callbacks=[EarlyStopping(patience=7),\n",
    "                                            History(),\n",
    "                                            metrics_callback],\n",
    "                                  validation_data=(data.X_valid,\n",
    "                                                   data.y_valid))\n",
    "    \n",
    "    #Visualize the model's performance curve \n",
    "    plot_learning_curve(history)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute model performance on a held out test set \n",
    "def compute_performance(model,data):\n",
    "    test_predictions=model.predict(data.X_test)\n",
    "    ## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "    print(ClassificationResult(data.y_test,test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos_and_neg_validation_sample(data,pos_index,neg_index):\n",
    "#get the indices of the specified positive and negative examples in the validation data split\n",
    "    pos_index=np.flatnonzero(data.y_valid==1)[pos_index]\n",
    "    pos_X=data.X_valid[pos_index:pos_index+1]\n",
    "    neg_index=np.flatnonzero(data.y_valid==0)[neg_index]\n",
    "    neg_X=data.X_valid[neg_index:neg_index+1]\n",
    "    return pos_X,neg_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize motif scores for a positive and negative example \n",
    "#pos_index: index value of a simulated positive datapoint to use for motif visualization \n",
    "#neg_index: index value of a simulated negative datapoint to use for motif visualization \n",
    "def visualize_motif_scores(model,pos_X,neg_X,data): \n",
    "    pos_motif_scores=get_motif_scores(pos_X,data.motif_names,return_positions=True).squeeze()\n",
    "    neg_motif_scores=get_motif_scores(neg_X,data.motif_names,return_positions=True).squeeze()\n",
    "    \n",
    "    plt.figure(figsize=(20,3))\n",
    "    ax1=plt.subplot(211)\n",
    "    ax2=plt.subplot(212)\n",
    "    \n",
    "    ax1.plot(pos_motif_scores, \"-o\")\n",
    "    ax1.set_ylim(0,max(pos_motif_scores))\n",
    "    ax1.set_xlabel(\"Sequence base\")\n",
    "    ax1.set_ylabel(\"Motif scan score\")\n",
    "    ax1.set_title(\"Positive example\")\n",
    "\n",
    "    ax2.plot(neg_motif_scores, \"-o\")\n",
    "    ax2.set_ylim(0,max(pos_motif_scores))\n",
    "    ax2.set_xlabel(\"Sequence base\")\n",
    "    ax2.set_ylabel(\"Motif scan score\")\n",
    "    ax2.set_title(\"Negative example\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import in_silico_mutagenesis, plot_ism\n",
    "\n",
    "def perform_ism(model,pos_X,neg_X,data): \n",
    "    ism_pos=in_silico_mutagenesis(model,pos_X)\n",
    "    ism_neg=in_silico_mutagenesis(model,neg_X)\n",
    "    min_val=min([np.amin(ism_pos),np.amin(ism_neg)])\n",
    "    max_val=max([np.amax(ism_pos),np.amax(ism_neg)])\n",
    "    plot_ism(ism_pos,\"Positive Example\",min_val=min_val,max_val=max_val)\n",
    "    plot_ism(ism_neg,\"Negative Example\",min_val=min_val,max_val=max_val)\n",
    "    return ism_pos, ism_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import input_grad\n",
    "\n",
    "def compute_gradxinput_scores(model,pos_X,neg_X,data):\n",
    "    gradinput_pos=input_grad(model,pos_X)\n",
    "    gradinput_neg=input_grad(model,neg_X)\n",
    "    min_val=min([np.amin(gradinput_pos),np.amin(gradinput_neg)])\n",
    "    max_val=max([np.amax(gradinput_pos),np.amax(gradinput_neg)])\n",
    "    plot_seq_importance(gradinput_pos,pos_X,title=\"Positive\",ylim=(min_val,max_val))\n",
    "    plot_seq_importance(gradinput_neg,neg_X,title=\"Negative\",ylim=(min_val,max_val))\n",
    "    return gradinput_pos,gradinput_neg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import deeplift\n",
    "\n",
    "def compute_deeplift_scores(model,pos_X,neg_X,data):\n",
    "    dl_pos=deeplift(model,pos_X)\n",
    "    dl_neg=deeplift(model,neg_X)\n",
    "    min_val=min([np.amin(gradinput_pos),np.amin(gradinput_neg)])\n",
    "    max_val=max([np.amax(gradinput_pos),np.amax(gradinput_neg)])\n",
    "    plot_seq_importance(dl_pos,pos_X,title=\"Positive\",ylim=(min_val,max_val))\n",
    "    plot_seq_importance(dl_neg,neg_X,title=\"Negative\",ylim=(min_val,max_val))\n",
    "    return dl_pos,dl_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret(model,pos_X,neg_X,data): \n",
    "    visualize_motif_scores(model,pos_X,neg_X,data)\n",
    "    ism_pos,ism_neg=perform_ism(model,pos_X,neg_X,data)\n",
    "    gradinput_pos,gradinput_neg=compute_gradxinput_scores(model,pos_X,neg_X,data)\n",
    "    dl_pos,dl_neg=compute_deeplift_scores(model,pos_X,neg_X,data)\n",
    "    return ism_pos, ism_neg, gradinput_pos, gradinput_neg, dl_pos, dl_neg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the full data analysis for a given dataset and model \n",
    "#Train the model, compute it's performance on a positive and negative data point \n",
    "def analyze(model,data,pos_index,neg_index):\n",
    "    model=train_model(model,data)\n",
    "    compute_performance(model,data)\n",
    "    pos_X,neg_X=extract_pos_and_neg_validation_sample(data,pos_index,neg_index)\n",
    "    ism_pos,ism_neg, gradinput_pos, gradinput_neg, dl_pos,dl_neg=interpret(model,pos_X,neg_X,data)\n",
    "    return pos_X, neg_X, ism_pos, ism_neg, gradinput_pos, gradinput_neg, dl_pos,dl_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set a random seed to ensure that all analyses in this tutorial are reproducible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure our results are reproducible\n",
    "from numpy.random import seed\n",
    "seed(1234)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting simulation data <a name='3'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "\n",
    "DragoNN provides a set of simulation functions. Let's use the **print_available_simulations** function to examine the list of simulations supported by DragoNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_available_simulations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Motif <a name='4'>\n",
    "<a href=#outline>Home</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with single motif detection of the TAL1_known4 motif: \n",
    "\n",
    "![play button](./primer_tutorial_images/TAL1_known4.png)\n",
    "Let's find out what parameters are needed for the simulation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_simulation_info(\"simulate_single_motif_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this binary simulation task, we simulate a negative set of 10K 500 bp random sequences and a positive set of 10K 500 bp random sequences with one instance of the TAL1 motif randomly embedded at any position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define simulation parameters \n",
    "tal1_parameters = {\n",
    "    \"motif_name\": \"TAL1_known4\",\n",
    "    \"seq_length\": 500, \n",
    "    \"num_pos\": 10000,\n",
    "    \"num_neg\": 10000,\n",
    "    \"GC_fraction\": 0.4}\n",
    "\n",
    "#Get simulation data \n",
    "tal1_data = get_simulation_data(\"simulate_single_motif_detection\",\n",
    "                                      tal1_parameters,\n",
    "                                      validation_set_size=3200, test_set_size=4000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the convolutional neural network model architecture: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimal model architecture in keras (Refer to Primer Tutorial 2)\n",
    "tal1_model=Sequential() \n",
    "tal1_model.add(Conv2D(filters=10,kernel_size=(1,15),input_shape=tal1_data.X_train.shape[1::]))\n",
    "tal1_model.add(Activation('relu'))\n",
    "tal1_model.add(MaxPooling2D(pool_size=(1,35)))\n",
    "tal1_model.add(Flatten())\n",
    "tal1_model.add(Dense(1))\n",
    "tal1_model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "tal1_model.compile(optimizer='adam',\n",
    "                               loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_X, neg_X, ism_pos, ism_neg, gradinput_pos, gradinput_neg, dl_pos, dl_neg=analyze(tal1_model,tal1_data,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in to the portion of the deepLIFT track with the strongest signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(dl_pos,pos_X,xlim=(220,275),title=\"Positive Zoomed\")\n",
    "plot_seq_importance(dl_neg,neg_X,xlim=(220,275),title=\"Negative Zoomed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homotypic motif density detection <a name='5'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define simulation parameters \n",
    "density_detection_parameters={\n",
    "    \"motif_name\": \"TAL1_known4\",\n",
    "    \"seq_length\": 500,\n",
    "    \"neg_counts\":[0,2],\n",
    "    \"pos_counts\":[3,5],\n",
    "    \"num_pos\": 10000,\n",
    "    \"num_neg\": 10000,\n",
    "    \"GC_fraction\":0.4\n",
    "}\n",
    "\n",
    "#Get simulation data\n",
    "density_detection_data=get_simulation_data(\"simulate_motif_counting\",\n",
    "                               density_detection_parameters,\n",
    "                               validation_set_size=3200,test_set_size=4000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimal model architecture in keras (Refer to Primer Tutorial 2)\n",
    "density_detection_model=Sequential() \n",
    "density_detection_model.add(Conv2D(filters=10,kernel_size=(1,15),input_shape=density_detection_data.X_train.shape[1::]))\n",
    "density_detection_model.add(Activation('relu'))\n",
    "density_detection_model.add(MaxPooling2D(pool_size=(1,35)))\n",
    "density_detection_model.add(Flatten())\n",
    "density_detection_model.add(Dense(1))\n",
    "density_detection_model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "density_detection_model.compile(optimizer='adam',\n",
    "                               loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_X, neg_X, ism_pos, ism_neg, gradinput_pos, gradinput_neg, dl_pos, dl_neg=analyze(density_detection_model,density_detection_data,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(dl_pos,pos_X,xlim=(200,350),title=\"Positive, Zoomed\")\n",
    "plot_seq_importance(dl_neg,neg_X,xlim=(200,350),title=\"Negative, Zoomed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homotypic motif density localization <a name='6'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define simulation parameters \n",
    "density_localization_parameters = {\n",
    "    \"motif_name\": \"TAL1_known4\",\n",
    "    \"seq_length\": 1000,\n",
    "    \"center_size\": 150,\n",
    "    \"min_motif_counts\": 2,\n",
    "    \"max_motif_counts\": 4, \n",
    "    \"num_pos\": 10000,\n",
    "    \"num_neg\": 10000,\n",
    "    \"GC_fraction\": 0.4}\n",
    "\n",
    "#Get simulation data\n",
    "density_localization_data=get_simulation_data(\"simulate_motif_density_localization\",\n",
    "                               density_localization_parameters,\n",
    "                               validation_set_size=3200,test_set_size=4000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimal model architecture in keras (Refer to Primer Tutorial 2)\n",
    "density_localization_model=Sequential() \n",
    "density_localization_model.add(Conv2D(filters=5,kernel_size=(1,10),input_shape=density_localization_data.X_train.shape[1::]))\n",
    "density_localization_model.add(Activation('relu'))\n",
    "density_localization_model.add(MaxPooling2D(pool_size=(1,10)))\n",
    "density_localization_model.add(Flatten())\n",
    "density_localization_model.add(Dense(1))\n",
    "density_localization_model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "density_localization_model.compile(optimizer='adam',\n",
    "                               loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_X, neg_X, ism_pos, ism_neg, gradinput_pos, gradinput_neg, dl_pos, dl_neg=analyze(density_localization_model,density_localization_data,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(dl_pos,pos_X,xlim=(220,275),title=\"Positive\")\n",
    "plot_seq_importance(dl_neg,neg_X,xlim=(220,275),title=\"Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple motifs (multi-task)<a name='7'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define simulation parameters \n",
    "multi_motif_parameters = {\n",
    "    \"motif_names\": [\"CTCF_known1\",\"ZNF143_known2\",\"SIX5_known1\"],\n",
    "    \"seq_length\": 500,\n",
    "    \"min_num_motifs\": 0,\n",
    "    \"max_num_motifs\": 1, \n",
    "    \"num_seqs\": 20000,\n",
    "    \"GC_fraction\": 0.4}\n",
    "\n",
    "#Get simulation data\n",
    "multi_motif_data=get_simulation_data(\"simulate_multi_motif_embedding\",\n",
    "                               multi_motif_parameters,\n",
    "                               validation_set_size=3200,test_set_size=4000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimal model architecture in keras (Refer to Primer Tutorial 2)\n",
    "multi_motif_model=Sequential() \n",
    "multi_motif_model.add(Conv2D(filters=20,kernel_size=(1,20),input_shape=multi_motif_data.X_train.shape[1::]))\n",
    "multi_motif_model.add(Activation('relu'))\n",
    "multi_motif_model.add(MaxPooling2D(pool_size=(1,10)))\n",
    "multi_motif_model.add(Flatten())\n",
    "multi_motif_model.add(Dense(3))\n",
    "multi_motif_model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "multi_motif_model.compile(optimizer='adam',\n",
    "                               loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_X, neg_X, ism_pos, ism_neg, gradinput_pos, gradinput_neg, dl_pos, dl_neg=analyze(multi_motif_model, multi_motif_data,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(dl_pos,pos_X,xlim=(220,275),title=\"Positive\")\n",
    "plot_seq_importance(dl_neg,neg_X,xlim=(220,275),title=\"Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heterotypic motifs spatial grammar<a name='8'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define simulation parameters \n",
    "heterodimer_parameters = {\n",
    "    \"motif1\": \"SPI1_known4\",\n",
    "    \"motif2\": \"IRF_known1\",\n",
    "    \"seq_length\": 500,\n",
    "    \"min_spacing\": 2,\n",
    "    \"max_spacing\": 5, \n",
    "    \"num_pos\": 10000,\n",
    "    \"num_neg\": 10000,\n",
    "    \"GC_fraction\": 0.4}\n",
    "\n",
    "#Get simulation data\n",
    "heterodimer_data=get_simulation_data(\"simulate_heterodimer_grammar\",\n",
    "                               heterodimer_parameters,\n",
    "                               validation_set_size=3200,test_set_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodimer_model=Sequential()\n",
    "heterodimer_model.add(Conv2D(filters=15,kernel_size=(1,15),input_shape=input_shape))\n",
    "heterodimer_model.add(Activation(\"relu\"))\n",
    "heterodimer_model.add(Conv2D(filters=15,kernel_size=(1,15),input_shape=input_shape))\n",
    "heterodimer_model.add(Activation(\"relu\"))\n",
    "heterodimer_model.add(Conv2D(filters=15,kernel_size=(1,15),input_shape=input_shape))\n",
    "heterodimer_model.add(Activation(\"relu\"))\n",
    "heterodimer_model.add(MaxPooling2D(pool_size=(1,35)))    \n",
    "heterodimer_model.add(Flatten())\n",
    "heterodimer_model.add(Dense(num_tasks))\n",
    "heterodimer_model.add(Activation(\"sigmoid\"))\n",
    "heterodimer_model.compile(optimizer='adam',loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_X, neg_X, ism_pos, ism_neg, gradinput_pos, gradinput_neg, dl_pos, dl_neg=analyze(heterodimer_model,heterodimer_data,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(dl_pos,pos_X,xlim=(220,275),title=\"Positive\")\n",
    "plot_seq_importance(dl_neg,neg_X,xlim=(220,275),title=\"Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions<a name='9'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
