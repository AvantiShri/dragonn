{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train your DragoNN tutorial 5: \n",
    "## Functional variant characterization for non-coding SNPs within the SPI1 motif \n",
    "\n",
    "This tutorial is a supplement to the DragoNN manuscript. \n",
    "\n",
    "This tutorial will take 2 - 3 hours if executed on a GPU.\n",
    "\n",
    "## Outline<a name='outline'>\n",
    "<ol>\n",
    "    <li><a href=#1>Input data: SPI1 ChiP-seq and experimental bQTL data</a></li>\n",
    "    <li><a href=#2>Genomewide classification and regression labels for SPI1 TF ChiPseq</a></li>\n",
    "    <li><a href=#3>Optional: Download pre-generated models and test-set predictions</a></li>\n",
    "    <li><a href=#4>Genome-wide classification for SPI1</a></li>\n",
    "    <li><a href=#5>Genome-wide regression for SPI1</a></li> \n",
    "    <li><a href=#6>Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT</a></li>\n",
    "    <li><a href=#7>Recovering bQTL effect sizes: Classification vs Regression</a></li>\n",
    "    <li><a href=#8>Model-predicted SNP effect sizes vs bQTL effect sizes</a></li>\n",
    "    <li><a href=#9>Conclusions</a></li>    \n",
    "    <li><a href=#10>Save tutorial outputs</a></li>\n",
    "</ol>\n",
    "Github issues on the [dragonn repository](https://github.com/kundajelab/dragonn) with feedback, questions, and discussion are always welcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the lines below if you are running this tutorial from Google Colab \n",
    "#!pip install https://github.com/kundajelab/simdna/archive/0.3.zip\n",
    "#!pip install https://github.com/kundajelab/dragonn/archive/keras_2.2_tensorflow_1.6_purekeras.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/annashch/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/users/annashch/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Making sure our results are reproducible\n",
    "from numpy.random import seed\n",
    "seed(1234)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/users/annashch/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#load dragonn tutorial utilities \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from dragonn.tutorial_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data <a name='1'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "This tutorial uses the same in vivo SPI1 transcription factor CHiP-seq dataset that was used in [Tutorial 4](https://colab.research.google.com/github/kundajelab/dragonn/blob/keras_2.2_tensorflow_1.6_purekeras/paper_supplement/PrimerTutorial%204%20-%20Interpreting%20predictive%20sequence%20features%20in%20in-vivo%20TF%20binding%20events.ipynb). Our goal is to compare predicted variant effect sizes from classification and regression models against experimental bQTL data. The bQTL data in this way serves as a \"gold-standard\" validation that in silico mutagenesis on the deep learning inputs leads to correct variant effect size prediction.  We  will use bQTL data  that has been intersected with SPI1 CISBP genome motif annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPI1, optimal IDR thresholded peaks, Myers lab, hg19\n",
    "# https://www.encodeproject.org/experiments/ENCSR000BGQ/\n",
    "!wget https://www.encodeproject.org/files/ENCFF002CHQ/@@download/ENCFF002CHQ.bed.gz\n",
    "\n",
    "## Download the hg19 chromsizes file (We only use chroms 1 -22, X, Y for training)\n",
    "!wget https://github.com/kundajelab/dragonn/blob/keras_2.2_tensorflow_1.6_purekeras/paper_supplement/hg19.chrom.sizes\n",
    "    \n",
    "## Download the hg19 fasta reference genome (and corresponding .fai index)\n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz\n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.fai \n",
    "\n",
    "    \n",
    "#Note: The BigWig signal files for this dataset are not merged across replicates. You can perform the merge\n",
    "# yourself if you have ucsctools available on your compute environment by running the commands below. Alternatively, \n",
    "# download the pre-merged signal bigWig file (also below). \n",
    "\n",
    "#To download the pre-merged signal bigWig file: \n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.TF.GM12878.bigWig\n",
    "\n",
    "#To regenerate this file from source ENCODE files, run: \n",
    "#!wget https://www.encodeproject.org/files/ENCFF000OBR/@@download/ENCFF000OBR.bigWig ## raw bigwig signal for rep 2\n",
    "#!wget https://www.encodeproject.org/files/ENCFF000OBT/@@download/ENCFF000OBT.bigWig ## # raw bigwig signal for rep 3 \n",
    "#!bigWigMerge ENCFF000OBR.bigWig ENCFF000OBT.bigWig SPI1.TF.GM12878.bedGraph ##merge the bigwig signal files \n",
    "#!bedGraphToBigWig SPI1.TF.GM12878.bedGraph hg19.chrom.sizes SPI1.TF.GM12878.bigWig \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download bQTL experimental data for SPI1 loci \n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.bQTLs.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating genome-wide classification and regression labels <a name='2'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the *genomewide_labels* function from the  [seqdataloader](https://github.com/kundajelab/seqdataloader) package to generate positive and negative labels for the TF-ChIPseq peaks across the genome. We will treat each sample as a task for the model and compare the performance of the model on SPI1 task in the single-tasked and multi-tasked setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqdataloader import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPI1\tENCFF002CHQ.bed.gz\tSPI1.TF.GM12878.bigWig\r\n"
     ]
    }
   ],
   "source": [
    "## seqdataloader accepts an input file, which we call SPI1.tasks.tsv, with task names in column 1, corresponding\n",
    "## peak files in column 2, and the signal track in column 3. In this tutorial, the task file will have a single task entry for the SPI1 TF CHiP-seq\n",
    "! echo \"SPI1\\tENCFF002CHQ.bed.gz\\tSPI1.TF.GM12878.bigWig\" > SPI1.task.tsv\n",
    "!cat SPI1.task.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the parameter configuration below, seqdataloader splits the genome into 1kb regions, with a stride of 50. Each 1kb region is centered at a 200 bp bin, with a left flank of 400 bases and a right flank of 400 bases. \n",
    "\n",
    "* In the classification case, each 200 bp bin is labeled as positive if a narrowPeak summit overlaps with it. The bin is labeled negative if there is no overlap with the narrowPeak. \n",
    "* In the regression case, the asinh(mean coverage) in the 200 bp bin is computed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The label generation may take 10 - 15 minutes to complete. If you prefer not to wait, you can download the \n",
    "pre-generated classification and regression labels for the training, validation, and test sets by uncommenting the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regression labels \n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.train.regression.hdf5\n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.valid.regression.hdf5\n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.test.regression.hdf5\n",
    "\n",
    "## Regression labels \n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.train.regression.hdf5\n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.valid.regression.hdf5\n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.test.regression.hdf5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer to generate the labels from scratch, execute the two code cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary of bed files and bigwig files for each task:\n",
      "SPI1\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr9with dimensions:(2824249, 4)\n",
      "got peak subset for chrom:chr9 for task:SPI1\n",
      "finished chromosome:chr9 for task:SPI1\n",
      "pre-allocated df for chrom:chr7with dimensions:(3182754, 4)\n",
      "got peak subset for chrom:chr7 for task:SPI1\n",
      "finished chromosome:chr7 for task:SPI1\n",
      "pre-allocated df for chrom:chr5with dimensions:(3618286, 4)\n",
      "pre-allocated df for chrom:chr3with dimensions:(3960429, 4)\n",
      "got peak subset for chrom:chr5 for task:SPI1\n",
      "got peak subset for chrom:chr3 for task:SPI1\n",
      "finished chromosome:chr5 for task:SPI1\n",
      "finished chromosome:chr3 for task:SPI1\n",
      "pre-allocated df for chrom:chr10with dimensions:(2710675, 4)\n",
      "got peak subset for chrom:chr10 for task:SPI1\n",
      "finished chromosome:chr10 for task:SPI1\n",
      "pre-allocated df for chrom:chr8with dimensions:(2927261, 4)\n",
      "pre-allocated df for chrom:chr6with dimensions:(3422282, 4)\n",
      "got peak subset for chrom:chr8 for task:SPI1\n",
      "finished chromosome:chr8 for task:SPI1\n",
      "got peak subset for chrom:chr6 for task:SPI1\n",
      "finished chromosome:chr6 for task:SPI1\n",
      "pre-allocated df for chrom:chr4with dimensions:(3823066, 4)\n",
      "got peak subset for chrom:chr4 for task:SPI1\n",
      "finished chromosome:chr4 for task:SPI1\n",
      "pre-allocated df for chrom:chr11with dimensions:(2700111, 4)\n",
      "got peak subset for chrom:chr11 for task:SPI1\n",
      "pre-allocated df for chrom:chr15with dimensions:(2050608, 4)\n",
      "finished chromosome:chr11 for task:SPI1\n",
      "got peak subset for chrom:chr15 for task:SPI1\n",
      "finished chromosome:chr15 for task:SPI1\n",
      "pre-allocated df for chrom:chr13with dimensions:(2303378, 4)\n",
      "pre-allocated df for chrom:chr17with dimensions:(1623885, 4)\n",
      "got peak subset for chrom:chr13 for task:SPI1\n",
      "finished chromosome:chr13 for task:SPI1\n",
      "got peak subset for chrom:chr17 for task:SPI1\n",
      "finished chromosome:chr17 for task:SPI1\n",
      "pre-allocated df for chrom:chr16with dimensions:(1807076, 4)\n",
      "pre-allocated df for chrom:chr12with dimensions:(2677018, 4)\n",
      "got peak subset for chrom:chr16 for task:SPI1\n",
      "finished chromosome:chr16 for task:SPI1\n",
      "got peak subset for chrom:chr12 for task:SPI1\n",
      "finished chromosome:chr12 for task:SPI1\n",
      "pre-allocated df for chrom:chr18with dimensions:(1561525, 4)\n",
      "pre-allocated df for chrom:chr14with dimensions:(2146971, 4)\n",
      "got peak subset for chrom:chr18 for task:SPI1\n",
      "finished chromosome:chr18 for task:SPI1\n",
      "got peak subset for chrom:chr14 for task:SPI1\n",
      "finished chromosome:chr14 for task:SPI1\n",
      "pre-allocated df for chrom:chr22with dimensions:(1026072, 4)\n",
      "got peak subset for chrom:chr22 for task:SPI1\n",
      "finished chromosome:chr22 for task:SPI1\n",
      "pre-allocated df for chrom:chr20with dimensions:(1260491, 4)\n",
      "got peak subset for chrom:chr20 for task:SPI1\n",
      "finished chromosome:chr20 for task:SPI1\n",
      "pre-allocated df for chrom:chr21with dimensions:(962578, 4)\n",
      "got peak subset for chrom:chr21 for task:SPI1\n",
      "finished chromosome:chr21 for task:SPI1\n",
      "pre-allocated df for chrom:chrXwith dimensions:(3105392, 4)\n",
      "got peak subset for chrom:chrX for task:SPI1\n",
      "finished chromosome:chrX for task:SPI1\n",
      "expanding chromosome pool outputs\n",
      "concatenating data frames for chromosomes\n",
      "writing output dataframe to disk\n",
      "done!\n",
      "creating dictionary of bed files and bigwig files for each task:\n",
      "SPI1\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr1with dimensions:(4984993, 4)\n",
      "got peak subset for chrom:chr1 for task:SPI1\n",
      "finished chromosome:chr1 for task:SPI1\n",
      "expanding chromosome pool outputs\n",
      "concatenating data frames for chromosomes\n",
      "writing output dataframe to disk\n",
      "done!\n",
      "creating dictionary of bed files and bigwig files for each task:\n",
      "SPI1\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr19with dimensions:(1182560, 4)\n",
      "got peak subset for chrom:chr19 for task:SPI1\n",
      "finished chromosome:chr19 for task:SPI1\n",
      "pre-allocated df for chrom:chr2with dimensions:(4863968, 4)\n",
      "got peak subset for chrom:chr2 for task:SPI1\n",
      "finished chromosome:chr2 for task:SPI1\n",
      "expanding chromosome pool outputs\n",
      "concatenating data frames for chromosomes\n",
      "writing output dataframe to disk\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#  Generate genome-wide classification labels \n",
    "\n",
    "#1) Training set: all chromosomes with the exception of 1,2, and 19 in our training set. Also, the dataset does not\n",
    "# include chromosome Y, so we exclude it as well. \n",
    "\n",
    "train_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.train.classification.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19','chrY'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':4,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(train_set_params)\n",
    "\n",
    "#2) Validation set: Chromosome 1\n",
    "valid_set_params={'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.valid.classification.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':1,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(valid_set_params)\n",
    "\n",
    "#3) Test set: Chromosomes 2, 19 \n",
    "test_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.test.classification.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':2,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(test_set_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary of bed files and bigwig files for each task:\n",
      "SPI1\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr9with dimensions:(2824249, 4)\n",
      "starting chromosome:chr9 for task:SPI1\n",
      "pre-allocated df for chrom:chr7with dimensions:(3182754, 4)\n",
      "starting chromosome:chr7 for task:SPI1\n",
      "pre-allocated df for chrom:chr5with dimensions:(3618286, 4)\n",
      "finished chromosome:chr9 for task:SPI1\n",
      "starting chromosome:chr5 for task:SPI1\n",
      "pre-allocated df for chrom:chr3with dimensions:(3960429, 4)\n",
      "starting chromosome:chr3 for task:SPI1\n",
      "finished chromosome:chr7 for task:SPI1\n",
      "finished chromosome:chr5 for task:SPI1\n",
      "finished chromosome:chr3 for task:SPI1\n",
      "pre-allocated df for chrom:chr10with dimensions:(2710675, 4)\n",
      "starting chromosome:chr10 for task:SPI1\n",
      "finished chromosome:chr10 for task:SPI1\n",
      "pre-allocated df for chrom:chr8with dimensions:(2927261, 4)\n",
      "starting chromosome:chr8 for task:SPI1\n",
      "finished chromosome:chr8 for task:SPI1\n",
      "pre-allocated df for chrom:chr6with dimensions:(3422282, 4)\n",
      "starting chromosome:chr6 for task:SPI1\n",
      "pre-allocated df for chrom:chr4with dimensions:(3823066, 4)\n",
      "starting chromosome:chr4 for task:SPI1\n",
      "pre-allocated df for chrom:chr11with dimensions:(2700111, 4)\n",
      "starting chromosome:chr11 for task:SPI1\n",
      "finished chromosome:chr6 for task:SPI1\n",
      "pre-allocated df for chrom:chr13with dimensions:(2303378, 4)\n",
      "starting chromosome:chr13 for task:SPI1\n",
      "finished chromosome:chr11 for task:SPI1\n",
      "finished chromosome:chr4 for task:SPI1\n",
      "finished chromosome:chr13 for task:SPI1\n",
      "pre-allocated df for chrom:chr15with dimensions:(2050608, 4)\n",
      "starting chromosome:chr15 for task:SPI1\n",
      "pre-allocated df for chrom:chr17with dimensions:(1623885, 4)\n",
      "finished chromosome:chr15 for task:SPI1\n",
      "starting chromosome:chr17 for task:SPI1\n",
      "pre-allocated df for chrom:chr12with dimensions:(2677018, 4)\n",
      "finished chromosome:chr17 for task:SPI1\n",
      "starting chromosome:chr12 for task:SPI1\n",
      "pre-allocated df for chrom:chr14with dimensions:(2146971, 4)\n",
      "starting chromosome:chr14 for task:SPI1\n",
      "pre-allocated df for chrom:chr16with dimensions:(1807076, 4)\n",
      "starting chromosome:chr16 for task:SPI1\n",
      "finished chromosome:chr12 for task:SPI1\n",
      "finished chromosome:chr14 for task:SPI1\n",
      "pre-allocated df for chrom:chr18with dimensions:(1561525, 4)\n",
      "starting chromosome:chr18 for task:SPI1\n",
      "finished chromosome:chr16 for task:SPI1\n",
      "finished chromosome:chr18 for task:SPI1\n",
      "pre-allocated df for chrom:chr22with dimensions:(1026072, 4)\n",
      "starting chromosome:chr22 for task:SPI1\n",
      "pre-allocated df for chrom:chr20with dimensions:(1260491, 4)\n",
      "starting chromosome:chr20 for task:SPI1\n",
      "finished chromosome:chr22 for task:SPI1\n",
      "finished chromosome:chr20 for task:SPI1\n",
      "pre-allocated df for chrom:chr21with dimensions:(962578, 4)\n",
      "starting chromosome:chr21 for task:SPI1\n",
      "finished chromosome:chr21 for task:SPI1\n",
      "pre-allocated df for chrom:chrXwith dimensions:(3105392, 4)\n",
      "starting chromosome:chrX for task:SPI1\n",
      "finished chromosome:chrX for task:SPI1\n",
      "expanding chromosome pool outputs\n",
      "concatenating data frames for chromosomes\n",
      "writing output dataframe to disk\n",
      "done!\n",
      "creating dictionary of bed files and bigwig files for each task:\n",
      "SPI1\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr1with dimensions:(4984993, 4)\n",
      "starting chromosome:chr1 for task:SPI1\n",
      "finished chromosome:chr1 for task:SPI1\n",
      "expanding chromosome pool outputs\n",
      "concatenating data frames for chromosomes\n",
      "writing output dataframe to disk\n",
      "done!\n",
      "creating dictionary of bed files and bigwig files for each task:\n",
      "SPI1\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr19with dimensions:(1182560, 4)\n",
      "starting chromosome:chr19 for task:SPI1\n",
      "finished chromosome:chr19 for task:SPI1\n",
      "pre-allocated df for chrom:chr2with dimensions:(4863968, 4)\n",
      "starting chromosome:chr2 for task:SPI1\n",
      "finished chromosome:chr2 for task:SPI1\n",
      "expanding chromosome pool outputs\n",
      "concatenating data frames for chromosomes\n",
      "writing output dataframe to disk\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Generate regression labels genome-wide \n",
    "\n",
    "#1) Training set: all chromosomes with the exception of 1,2, and 19 in our training set \n",
    "\n",
    "train_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.train.regression.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19','chrY'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':4,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(train_set_params)\n",
    "\n",
    "#2) Validation set: Chromosome 1\n",
    "valid_set_params={'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.valid.regression.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':1,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(valid_set_params)\n",
    "\n",
    "#3) Test set: Chromosomes 2, 19 \n",
    "test_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.test.regression.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':2,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(test_set_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the files that were generated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>SPI1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHR</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">chr3</th>\n",
       "      <th>50000000</th>\n",
       "      <th>50001000</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000050</th>\n",
       "      <th>50001050</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000100</th>\n",
       "      <th>50001100</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000150</th>\n",
       "      <th>50001150</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000200</th>\n",
       "      <th>50001200</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000250</th>\n",
       "      <th>50001250</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000300</th>\n",
       "      <th>50001300</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000350</th>\n",
       "      <th>50001350</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000400</th>\n",
       "      <th>50001400</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000450</th>\n",
       "      <th>50001450</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        SPI1\n",
       "CHR  START    END           \n",
       "chr3 50000000 50001000   0.0\n",
       "     50000050 50001050   0.0\n",
       "     50000100 50001100   0.0\n",
       "     50000150 50001150   0.0\n",
       "     50000200 50001200   0.0\n",
       "     50000250 50001250   0.0\n",
       "     50000300 50001300   0.0\n",
       "     50000350 50001350   0.0\n",
       "     50000400 50001400   0.0\n",
       "     50000450 50001450   0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The code generates bed file outputs with a label of 1 or 0 for each 1kb\n",
    "# genome bin for each task. Note that the bins are shifted with a stride of 50.\n",
    "pd.read_hdf(\"SPI1.train.classification.hdf5\",start=1000000,stop=1000010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>SPI1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHR</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">chr3</th>\n",
       "      <th>50000000</th>\n",
       "      <th>50001000</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000050</th>\n",
       "      <th>50001050</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000100</th>\n",
       "      <th>50001100</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000150</th>\n",
       "      <th>50001150</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000200</th>\n",
       "      <th>50001200</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000250</th>\n",
       "      <th>50001250</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000300</th>\n",
       "      <th>50001300</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000350</th>\n",
       "      <th>50001350</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000400</th>\n",
       "      <th>50001400</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000450</th>\n",
       "      <th>50001450</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        SPI1\n",
       "CHR  START    END           \n",
       "chr3 50000000 50001000   0.0\n",
       "     50000050 50001050   0.0\n",
       "     50000100 50001100   0.0\n",
       "     50000150 50001150   0.0\n",
       "     50000200 50001200   0.0\n",
       "     50000250 50001250   0.0\n",
       "     50000300 50001300   0.0\n",
       "     50000350 50001350   0.0\n",
       "     50000400 50001400   0.0\n",
       "     50000450 50001450   0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_hdf(\"SPI1.train.regression.hdf5\",start=1000000,stop=1000010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Download pre-generated models and test-set predictions <a name='3'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "Next, we will train classification and regression models to predict TF CHiP-seq peaks for SPI1. If you want to skip straight to model interpretation and bQTL analysis, you can download the pre-trained models by uncommenting the \n",
    "block of code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model \n",
    "\n",
    "## Download classification model \n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.classification.model.hdf5\n",
    "# spi1_classification_model=load_model(\"SPI1.classification.model.hdf5\")\n",
    "\n",
    "## Download regression model \n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.regression.model.hdf5\n",
    "# spi1_regression_model=load_model(\"SPI1.regression.model.hdf5\")\n",
    "\n",
    "\n",
    "## Get test set classification model and regression model predictions \n",
    "#import h5py\n",
    "#test_set_predictions=h5py.File(\"SPI1.test.predictions.hdf5\")\n",
    "#spi1_test_classification_predictions=test_set_predictions['classification'].value \n",
    "#spi1_test_regression_predictions=test_set_predictions['regression'].value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome-wide classification model <a name='4'>\n",
    "<a href=#outline>Home</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To prepare for model training, we import the necessary functions and submodules from keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta, SGD, RMSprop;\n",
    "import keras.losses;\n",
    "from keras.constraints import maxnorm;\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping, History\n",
    "from keras import backend as K \n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concise.metrics import tpr, tnr, fpr, fnr, precision, f1\n",
    "def initialize_classification_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=(1,1000,4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(1,35)))\n",
    "\n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(ntasks))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',\n",
    "                  metrics=[tpr,\n",
    "                           tnr,\n",
    "                           fpr,\n",
    "                           fnr,\n",
    "                           precision,\n",
    "                           f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create generators for the training and validation data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators, upsample positives to ensure they constitute 30% of each batch \n",
    "from dragonn.generators import * \n",
    "spi1_train_classification_gen=DataGenerator(\"SPI1.train.classification.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3)\n",
    "spi1_valid_classification_gen=DataGenerator(\"SPI1.valid.classification.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "10000/10000 [==============================] - 187s 19ms/step - loss: 0.3670 - sensitivity: 0.6436 - specificity: 0.9128 - fpr: 0.0872 - fnr: 0.3564 - precision: nan - f1: nan - val_loss: 0.2435 - val_sensitivity: 0.8012 - val_specificity: 0.9445 - val_fpr: 0.0555 - val_fnr: 0.1988 - val_precision: 0.8633 - val_f1: 0.8280\n",
      "Epoch 2/150\n",
      "10000/10000 [==============================] - 178s 18ms/step - loss: 0.2559 - sensitivity: 0.8103 - specificity: 0.9310 - fpr: 0.0690 - fnr: 0.1897 - precision: 0.8378 - f1: 0.8205 - val_loss: 0.2191 - val_sensitivity: 0.8031 - val_specificity: 0.9549 - val_fpr: 0.0451 - val_fnr: 0.1969 - val_precision: 0.8863 - val_f1: 0.8398\n",
      "Epoch 3/150\n",
      "10000/10000 [==============================] - 174s 17ms/step - loss: 0.2391 - sensitivity: 0.8294 - specificity: 0.9343 - fpr: 0.0657 - fnr: 0.1706 - precision: 0.8472 - f1: 0.8352 - val_loss: 0.2202 - val_sensitivity: 0.8920 - val_specificity: 0.9258 - val_fpr: 0.0742 - val_fnr: 0.1080 - val_precision: 0.8399 - val_f1: 0.8631\n",
      "Epoch 4/150\n",
      "10000/10000 [==============================] - 173s 17ms/step - loss: 0.2367 - sensitivity: 0.8308 - specificity: 0.9361 - fpr: 0.0639 - fnr: 0.1692 - precision: 0.8511 - f1: 0.8379 - val_loss: 0.2147 - val_sensitivity: 0.7974 - val_specificity: 0.9618 - val_fpr: 0.0382 - val_fnr: 0.2026 - val_precision: 0.9015 - val_f1: 0.8434\n",
      "Epoch 5/150\n",
      "   13/10000 [..............................] - ETA: 2:24 - loss: 0.2315 - sensitivity: 0.8178 - specificity: 0.9470 - fpr: 0.0530 - fnr: 0.1822 - precision: 0.8735 - f1: 0.8412"
     ]
    }
   ],
   "source": [
    "#Train the SPI1 classification model \n",
    "spi1_classification_model=initialize_classification_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_classification=spi1_classification_model.fit_generator(spi1_train_classification_gen,\n",
    "                                                  validation_data=spi1_valid_classification_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=10000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "from dragonn.tutorial_utils import plot_learning_curve\n",
    "plot_learning_curve(history_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now measure how well the model performed by calculating performance metrics on the test splits across the whole genome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_classification_gen=DataGenerator(\"SPI1.test.classification.hdf5\",\n",
    "                                       \"hg19.genome.fa.gz\",\n",
    "                                         upsample=False,\n",
    "                                         add_revcomp=False,\n",
    "                                         batch_size=1000,\n",
    "                                         tasks=['SPI1'])\n",
    "spi1_test_classification_predictions=spi1_classification_model.predict_generator(spi1_test_classification_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "spi1_test_classification_truth=spi1_test_classification_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_classification_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_classification_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "from dragonn.metrics import ClassificationResult\n",
    "print(ClassificationResult(spi1_test_classification_truth.values.astype(bool),spi1_test_classification_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome-wide regression model <a name='5'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concise.metrics import tpr, tnr, fpr, fnr, precision, f1\n",
    "def initialize_regression_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=(1,1000,4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(1,35)))\n",
    "\n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(ntasks))\n",
    "\n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to determine a threshold for upsampling the non-zero bins in a given batch \n",
    "# extract 5 million datapoints from the training data and observe the distribution of non-zero signal values  \n",
    "sample=pd.read_hdf(\"SPI1.train.regression.hdf5\",start=0,stop=5000000)\n",
    "nonzero_sample=sample[sample.max(axis=1)>0]\n",
    "print(nonzero_sample.shape)\n",
    "nonzero_sample.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that 0.1 is a reasonable threshold for upsampling non-zero bins in regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators, no upsampling of positives is used for regression. \n",
    "from dragonn.generators import * \n",
    "spi1_train_regression_gen=DataGenerator(\"SPI1.train.regression.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,upsample_thresh=0.1)\n",
    "spi1_valid_regression_gen=DataGenerator(\"SPI1.valid.regression.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,upsample_thresh=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 regression model \n",
    "spi1_regression_model=initialize_regression_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_regression=spi1_regression_model.fit_generator(spi1_train_regression_gen,\n",
    "                                                  validation_data=spi1_valid_regression_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=10000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(history_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_regression_gen=DataGenerator(\"SPI1.test.regression.hdf5\",\n",
    "                                       \"hg19.genome.fa.gz\",\n",
    "                                         upsample=False,\n",
    "                                         add_revcomp=False,\n",
    "                                         batch_size=1000,\n",
    "                                         tasks=['SPI1'])\n",
    "spi1_test_regression_predictions=spi1_regression_model.predict_generator(spi1_test_regression_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "spi1_test_regression_truth=spi1_test_regression_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate spearman and pearson correlation between truth labels and predictions \n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "corr_pearson=pearsonr(spi1_test_regression_truth,spi1_test_regression_predictions)\n",
    "corr_spearman=spearmanr(spi1_test_regression_truth,spi1_test_regression_predictions)\n",
    "print(\"Pearson correlation on test set:\"+str(corr_pearson))\n",
    "print(\"Spearman correlation on test set:\"+str(corr_spearman))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT <a name='6'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the true positive predictions with a threshold of 0.9 (i.e. high confidence true positive predictions)\n",
    "spi1_test_classification_truth_bool=spi1_test_classification_truth.values.astype(bool)\n",
    "true_pos_spi1=spi1_test_classification_truth[spi1_test_classification_truth_bool*spi1_test_classification_predictions >0.9]\n",
    "true_pos_spi1.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_spi1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.utils import one_hot_from_bed\n",
    "deep_lift_input_spi1=one_hot_from_bed([i for i in true_pos_spi1.index],\"hg19.genome.fa.gz\")\n",
    "deep_lift_input_spi1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import deeplift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1=deeplift(spi1_classification_model,deep_lift_input_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a few of the DeepLIFT tracks and see if the model successfully learned SPI1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import  plot_seq_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[0],deep_lift_input_spi1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[1],deep_lift_input_spi1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2],deep_lift_input[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in to the center of one sequence so that it is easier to distinguish the motif: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores[2].squeeze()[400:500],deep_lift_input[2].squeeze()[400:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we query the sequence \"GTTTCACTTC\" in the [TomTom](http://meme-suite.org/tools/tomtom) software from the MEME suite, we find that the motif is a good match for IRF2, which is in the same motif family (Tryptophan cluster factors) as SPI1: \n",
    "<img src=\"tutorial_images/IRF.Tut4.png\" alt=\"IRF2TomTom\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering bQTL effect sizes: Classification vs Regression <a name='7'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-predicted SNP effect sizes vs bQTL effect sizes <a name='8'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a name='9'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save tutorial outputs <a name='10'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "We save the models and test set predictions generated in this tutorial to an hdf5 file so that they can be loaded more readily in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the models \n",
    "spi1_classification_model.save(\"SPI1.classification.model.hdf5\")\n",
    "spi1_regression_model.save(\"SPI1.regression.model.hdf5\")\n",
    "#save the test predictions \n",
    "import h5py \n",
    "test_set_predictions=h5py.File(\"SPI1.test.predictions.hdf5\",'w')\n",
    "test_set_predictions.create_dataset(\"classification\",data=spi1_test_classification_predictions)\n",
    "test_set_predictions.create_dataset(\"regression\",data=spi1_test_regression_predictions)\n",
    "test_set_predictions.close() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
