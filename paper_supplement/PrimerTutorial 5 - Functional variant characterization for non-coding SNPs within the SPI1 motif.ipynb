{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train your DragoNN tutorial 5: \n",
    "## Functional variant characterization for non-coding SNPs within the SPI1 motif \n",
    "\n",
    "This tutorial is a supplement to the DragoNN manuscript. \n",
    "\n",
    "This tutorial will take 2 - 3 hours if executed on a GPU.\n",
    "\n",
    "## Outline<a name='outline'>\n",
    "<ol>\n",
    "    <li><a href=#1>Input data: SPI1 ChiP-seq and experimental bQTL data</a></li>\n",
    "    <li><a href=#2>Genomewide classification and regression labels for SPI1 TF ChiPseq</a></li>\n",
    "    <li><a href=#3>Optional: Download pre-generated models and test-set predictions</a></li>\n",
    "    <li><a href=#4>Genome-wide classification for SPI1</a></li>\n",
    "    <li><a href=#5>Genome-wide regression for SPI1</a></li> \n",
    "    <li><a href=#6>Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT</a></li>\n",
    "    <li><a href=#7>Recovering bQTL effect sizes: Classification vs Regression</a></li>\n",
    "    <li><a href=#8>Model-predicted SNP effect sizes vs bQTL effect sizes</a></li>\n",
    "    <li><a href=#9>Conclusions</a></li>    \n",
    "    <li><a href=#10>Save tutorial outputs</a></li>\n",
    "</ol>\n",
    "Github issues on the [dragonn repository](https://github.com/kundajelab/dragonn) with feedback, questions, and discussion are always welcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the lines below if you are running this tutorial from Google Colab \n",
    "#!pip install https://github.com/kundajelab/simdna/archive/0.3.zip\n",
    "#!pip install https://github.com/kundajelab/dragonn/archive/keras_2.2_tensorflow_1.6_purekeras.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/annashch/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/users/annashch/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Making sure our results are reproducible\n",
    "from numpy.random import seed\n",
    "seed(1234)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/users/annashch/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#load dragonn tutorial utilities \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from dragonn.tutorial_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data <a name='1'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "This tutorial uses the same in vivo SPI1 transcription factor CHiP-seq dataset that was used in [Tutorial 4](https://colab.research.google.com/github/kundajelab/dragonn/blob/keras_2.2_tensorflow_1.6_purekeras/paper_supplement/PrimerTutorial%204%20-%20Interpreting%20predictive%20sequence%20features%20in%20in-vivo%20TF%20binding%20events.ipynb). Our goal is to compare predicted variant effect sizes from classification and regression models against experimental bQTL data. The bQTL data in this way serves as a \"gold-standard\" validation that in silico mutagenesis on the deep learning inputs leads to correct variant effect size prediction.  We  will use bQTL data  that has been intersected with SPI1 CISBP genome motif annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-02-15 23:53:44--  http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/spi1_ENCSR000BGQ/cromwell-executions/chip/bb0c3c5a-3889-43fe-a218-05851cecc74a/call-macs2_pooled/execution/ENCFF000OBU.Rep1.merged.nodup.pooled_x_ENCFF000OCW.Control.Rep1.merged.nodup.fc.signal.bigwig\n",
      "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
      "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 793040708 (756M)\n",
      "Saving to: ‘SPI1.pooled.fc.bigWig’\n",
      "\n",
      "SPI1.pooled.fc.bigW 100%[===================>] 756.30M   111MB/s    in 9.8s    \n",
      "\n",
      "2019-02-15 23:53:54 (77.4 MB/s) - ‘SPI1.pooled.fc.bigWig’ saved [793040708/793040708]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SPI1, optimal IDR thresholded peaks, Myers lab, hg19\n",
    "# https://www.encodeproject.org/experiments/ENCSR000BGQ/\n",
    "#!wget -O SPI1.narrowPeak.gz http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/spi1_ENCSR000BGQ/cromwell-executions/chip/bb0c3c5a-3889-43fe-a218-05851cecc74a/call-reproducibility_idr/execution/optimal_peak.regionPeak.gz\n",
    "\n",
    "#Fold change bigWig track for the SPI1 dataset: \n",
    "!wget -O SPI1.pooled.fc.bigWig http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/spi1_ENCSR000BGQ/cromwell-executions/chip/bb0c3c5a-3889-43fe-a218-05851cecc74a/call-macs2_pooled/execution/ENCFF000OBU.Rep1.merged.nodup.pooled_x_ENCFF000OCW.Control.Rep1.merged.nodup.fc.signal.bigwig\n",
    "    \n",
    "## Download the hg19 chromsizes file (We only use chroms 1 -22, X, Y for training)\n",
    "#!wget https://github.com/kundajelab/dragonn/blob/keras_2.2_tensorflow_1.6_purekeras/paper_supplement/hg19.chrom.sizes\n",
    "    \n",
    "## Download the hg19 fasta reference genome (and corresponding .fai index)\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.fai \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-02-15 23:53:56--  http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.bQTLs.txt.gz\n",
      "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
      "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23850031 (23M) [application/x-gzip]\n",
      "Saving to: ‘SPI1.bQTLs.txt.gz.2’\n",
      "\n",
      "SPI1.bQTLs.txt.gz.2 100%[===================>]  22.75M  43.3MB/s    in 0.5s    \n",
      "\n",
      "2019-02-15 23:53:57 (43.3 MB/s) - ‘SPI1.bQTLs.txt.gz.2’ saved [23850031/23850031]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download bQTL experimental data for SPI1 loci \n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.bQTLs.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating genome-wide classification and regression labels <a name='2'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the *genomewide_labels* function from the  [seqdataloader](https://github.com/kundajelab/seqdataloader) package to generate positive and negative labels for the TF-ChIPseq peaks across the genome. We will treat each sample as a task for the model and compare the performance of the model on SPI1 task in the single-tasked and multi-tasked setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqdataloader import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPI1\tSPI1.narrowPeak.gz\tSPI1.pooled.fc.bigWig\r\n"
     ]
    }
   ],
   "source": [
    "## seqdataloader accepts an input file, which we call SPI1.tasks.tsv, with task names in column 1, corresponding\n",
    "## peak files in column 2, and the signal track in column 3. In this tutorial, the task file will have a single task entry for the SPI1 TF CHiP-seq\n",
    "! echo \"SPI1\\tSPI1.narrowPeak.gz\\tSPI1.pooled.fc.bigWig\" > SPI1.task.tsv\n",
    "!cat SPI1.task.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the parameter configuration below, seqdataloader splits the genome into 1kb regions, with a stride of 50. Each 1kb region is centered at a 200 bp bin, with a left flank of 400 bases and a right flank of 400 bases. \n",
    "\n",
    "* In the classification case, each 200 bp bin is labeled as positive if a narrowPeak summit overlaps with it. The bin is labeled negative if there is no overlap with the narrowPeak. \n",
    "* In the regression case, the asinh(mean coverage) in the 200 bp bin is computed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The label generation may take 10 - 15 minutes to complete. If you prefer not to wait, you can download the \n",
    "pre-generated classification and regression labels for the training, validation, and test sets by uncommenting the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regression labels \n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.train.regression.hdf5\n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.valid.regression.hdf5\n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.test.regression.hdf5\n",
    "\n",
    "## Regression labels \n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.train.regression.hdf5\n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.valid.regression.hdf5\n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.test.regression.hdf5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer to generate the labels from scratch, execute the two code cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary of bed files and bigwig files for each task:\n",
      "SPI1\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr9with dimensions:(2824249, 4)\n",
      "pre-allocated df for chrom:chr7with dimensions:(3182754, 4)\n",
      "got peak subset for chrom:chr7 for task:SPI1\n",
      "pre-allocated df for chrom:chr5with dimensions:(3618286, 4)\n",
      "finished chromosome:chr7 for task:SPI1got peak subset for chrom:chr9 for task:SPI1\n",
      "\n",
      "finished chromosome:chr9 for task:SPI1\n",
      "got peak subset for chrom:chr5 for task:SPI1\n",
      "pre-allocated df for chrom:chr3with dimensions:(3960429, 4)\n",
      "finished chromosome:chr5 for task:SPI1\n",
      "got peak subset for chrom:chr3 for task:SPI1\n",
      "finished chromosome:chr3 for task:SPI1\n",
      "pre-allocated df for chrom:chr8with dimensions:(2927261, 4)\n",
      "pre-allocated df for chrom:chr10with dimensions:(2710675, 4)\n",
      "pre-allocated df for chrom:chr6with dimensions:(3422282, 4)\n",
      "got peak subset for chrom:chr8 for task:SPI1\n",
      "got peak subset for chrom:chr10 for task:SPI1\n",
      "finished chromosome:chr8 for task:SPI1\n",
      "got peak subset for chrom:chr6 for task:SPI1\n",
      "finished chromosome:chr10 for task:SPI1\n",
      "finished chromosome:chr6 for task:SPI1\n",
      "pre-allocated df for chrom:chr4with dimensions:(3823066, 4)\n",
      "got peak subset for chrom:chr4 for task:SPI1\n",
      "finished chromosome:chr4 for task:SPI1\n",
      "pre-allocated df for chrom:chr13with dimensions:(2303378, 4)\n",
      "pre-allocated df for chrom:chr11with dimensions:(2700111, 4)\n",
      "got peak subset for chrom:chr13 for task:SPI1\n",
      "pre-allocated df for chrom:chr15with dimensions:(2050608, 4)\n",
      "finished chromosome:chr13 for task:SPI1\n",
      "got peak subset for chrom:chr11 for task:SPI1\n",
      "finished chromosome:chr11 for task:SPI1\n",
      "pre-allocated df for chrom:chr17with dimensions:(1623885, 4)\n",
      "got peak subset for chrom:chr15 for task:SPI1\n",
      "finished chromosome:chr15 for task:SPI1\n",
      "got peak subset for chrom:chr17 for task:SPI1\n",
      "finished chromosome:chr17 for task:SPI1\n",
      "pre-allocated df for chrom:chr16with dimensions:(1807076, 4)\n",
      "pre-allocated df for chrom:chr14with dimensions:(2146971, 4)\n",
      "pre-allocated df for chrom:chr18with dimensions:(1561525, 4)\n",
      "got peak subset for chrom:chr16 for task:SPI1\n",
      "finished chromosome:chr16 for task:SPI1\n",
      "pre-allocated df for chrom:chr12with dimensions:(2677018, 4)\n",
      "got peak subset for chrom:chr18 for task:SPI1\n",
      "got peak subset for chrom:chr14 for task:SPI1\n",
      "finished chromosome:chr18 for task:SPI1\n",
      "finished chromosome:chr14 for task:SPI1\n",
      "got peak subset for chrom:chr12 for task:SPI1\n",
      "finished chromosome:chr12 for task:SPI1\n",
      "pre-allocated df for chrom:chr20with dimensions:(1260491, 4)\n",
      "pre-allocated df for chrom:chr22with dimensions:(1026072, 4)\n",
      "got peak subset for chrom:chr20 for task:SPI1\n",
      "finished chromosome:chr20 for task:SPI1\n",
      "got peak subset for chrom:chr22 for task:SPI1\n",
      "finished chromosome:chr22 for task:SPI1\n",
      "pre-allocated df for chrom:chr21with dimensions:(962578, 4)\n",
      "got peak subset for chrom:chr21 for task:SPI1\n",
      "finished chromosome:chr21 for task:SPI1\n",
      "pre-allocated df for chrom:chrXwith dimensions:(3105392, 4)\n",
      "got peak subset for chrom:chrX for task:SPI1\n",
      "finished chromosome:chrX for task:SPI1\n",
      "expanding chromosome pool outputs\n",
      "concatenating data frames for chromosomes\n",
      "writing output dataframe to disk\n"
     ]
    }
   ],
   "source": [
    "#  Generate genome-wide classification labels \n",
    "\n",
    "#1) Training set: all chromosomes with the exception of 1,2, and 19 in our training set. Also, the dataset does not\n",
    "# include chromosome Y, so we exclude it as well. \n",
    "\n",
    "train_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.train.classification.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19','chrY'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':4,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(train_set_params)\n",
    "\n",
    "#2) Validation set: Chromosome 1\n",
    "valid_set_params={'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.valid.classification.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':1,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(valid_set_params)\n",
    "\n",
    "#3) Test set: Chromosomes 2, 19 \n",
    "test_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.test.classification.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':2,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(test_set_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression labels genome-wide \n",
    "\n",
    "#1) Training set: all chromosomes with the exception of 1,2, and 19 in our training set \n",
    "\n",
    "train_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.train.regression.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19','chrY'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':4,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(train_set_params)\n",
    "\n",
    "#2) Validation set: Chromosome 1\n",
    "valid_set_params={'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.valid.regression.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':1,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(valid_set_params)\n",
    "\n",
    "#3) Test set: Chromosomes 2, 19 \n",
    "test_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.test.regression.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':2,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(test_set_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the files that were generated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code generates bed file outputs with a label of 1 or 0 for each 1kb\n",
    "# genome bin for each task. Note that the bins are shifted with a stride of 50.\n",
    "pd.read_hdf(\"SPI1.train.classification.hdf5\",start=1000000,stop=1000010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_hdf(\"SPI1.train.regression.hdf5\",start=1000000,stop=1000010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Download pre-generated models and test-set predictions <a name='3'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "Next, we will train classification and regression models to predict TF CHiP-seq peaks for SPI1. If you want to skip straight to model interpretation and bQTL analysis, you can download the pre-trained models by uncommenting the \n",
    "block of code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model \n",
    "\n",
    "## Download classification model \n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.classification.model.hdf5\n",
    "#spi1_classification_model=load_model(\"SPI1.classification.model.hdf5\")\n",
    "\n",
    "## Download regression model \n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.regression.model.hdf5\n",
    "#spi1_regression_model=load_model(\"SPI1.regression.model.hdf5\")\n",
    "\n",
    "\n",
    "## Get test set classification model and regression model predictions \n",
    "#import h5py\n",
    "#test_set_predictions=h5py.File(\"SPI1.test.predictions.hdf5\")\n",
    "#spi1_test_classification_predictions=test_set_predictions['classification'].value \n",
    "#spi1_test_regression_predictions=test_set_predictions['regression'].value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome-wide classification model <a name='4'>\n",
    "<a href=#outline>Home</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To prepare for model training, we import the necessary functions and submodules from keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta, SGD, RMSprop;\n",
    "import keras.losses;\n",
    "from keras.constraints import maxnorm;\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping, History\n",
    "from keras import backend as K \n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concise.metrics import tpr, tnr, fpr, fnr, precision, f1\n",
    "def initialize_classification_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=(1,1000,4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(1,35)))\n",
    "\n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(ntasks))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',\n",
    "                  metrics=[tpr,\n",
    "                           tnr,\n",
    "                           fpr,\n",
    "                           fnr,\n",
    "                           precision,\n",
    "                           f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create generators for the training and validation data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators, upsample positives to ensure they constitute 30% of each batch \n",
    "from dragonn.generators import * \n",
    "spi1_train_classification_gen=DataGenerator(\"SPI1.train.classification.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3)\n",
    "spi1_valid_classification_gen=DataGenerator(\"SPI1.valid.classification.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 classification model \n",
    "spi1_classification_model=initialize_classification_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_classification=spi1_classification_model.fit_generator(spi1_train_classification_gen,\n",
    "                                                  validation_data=spi1_valid_classification_gen,\n",
    "                                                  steps_per_epoch=1000000,\n",
    "                                                  validation_steps=100000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "from dragonn.tutorial_utils import plot_learning_curve\n",
    "plot_learning_curve(history_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now measure how well the model performed by calculating performance metrics on the test splits across the whole genome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_classification_gen=DataGenerator(\"SPI1.test.classification.hdf5\",\n",
    "                                       \"hg19.genome.fa.gz\",\n",
    "                                         upsample=False,\n",
    "                                         add_revcomp=False,\n",
    "                                         batch_size=1000,\n",
    "                                         tasks=['SPI1'])\n",
    "spi1_test_classification_predictions=spi1_classification_model.predict_generator(spi1_test_classification_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "spi1_test_classification_truth=spi1_test_classification_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_classification_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_classification_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "from dragonn.metrics import ClassificationResult\n",
    "print(ClassificationResult(spi1_test_classification_truth.values.astype(bool),spi1_test_classification_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the models \n",
    "spi1_classification_model.save(\"SPI1.classification.model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome-wide regression model <a name='5'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_regression_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=(1,1000,4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(1,35)))\n",
    "\n",
    "    model.add(Conv2D(filters=10,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=5,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(ntasks))\n",
    "\n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to determine a threshold for upsampling the non-zero bins in a given batch \n",
    "# extract 5 million datapoints from the training data and observe the distribution of non-zero signal values  \n",
    "sample=pd.read_hdf(\"SPI1.train.regression.hdf5\",start=0,stop=5000000)\n",
    "nonzero_sample=sample[sample.max(axis=1)>0]\n",
    "print(nonzero_sample.shape)\n",
    "nonzero_sample.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that 0.1 is a reasonable threshold for upsampling non-zero bins in regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators, no upsampling of positives is used for regression. \n",
    "from dragonn.generators import * \n",
    "spi1_train_regression_gen=DataGenerator(\"SPI1.train.regression.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,upsample_thresh=0.1)\n",
    "spi1_valid_regression_gen=DataGenerator(\"SPI1.valid.regression.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,upsample_thresh=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 regression model \n",
    "spi1_regression_model=initialize_regression_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_regression=spi1_regression_model.fit_generator(spi1_train_regression_gen,\n",
    "                                                  validation_data=spi1_valid_regression_gen,\n",
    "                                                  steps_per_epoch=1000000,\n",
    "                                                  validation_steps=100000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(history_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_regression_gen=DataGenerator(\"SPI1.test.regression.hdf5\",\n",
    "                                       \"hg19.genome.fa.gz\",\n",
    "                                         upsample=False,\n",
    "                                         add_revcomp=False,\n",
    "                                         batch_size=1000,\n",
    "                                         tasks=['SPI1'])\n",
    "spi1_test_regression_predictions=spi1_regression_model.predict_generator(spi1_test_regression_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "spi1_test_regression_truth=spi1_test_regression_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate spearman and pearson correlation between truth labels and predictions \n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "corr_pearson=pearsonr(spi1_test_regression_truth,spi1_test_regression_predictions)\n",
    "corr_spearman=spearmanr(spi1_test_regression_truth,spi1_test_regression_predictions)\n",
    "print(\"Pearson correlation on test set:\"+str(corr_pearson))\n",
    "print(\"Spearman correlation on test set:\"+str(corr_spearman))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is some overfitting, let's save this model and see if we can do better \n",
    "spi1_regression_model.save(\"SPI1.regression.model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a fully connected layer for the regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_regression_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=(1,1000,4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling2D(pool_size=(1,35)))\n",
    "\n",
    "    model.add(Conv2D(filters=10,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(filters=5,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(500)) ###### NEW DENSE LAYER ADDED ##### \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(ntasks))\n",
    "\n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_train_regression_gen=DataGenerator(\"SPI1.train.regression.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,upsample_thresh=0.1)\n",
    "spi1_valid_regression_gen=DataGenerator(\"SPI1.valid.regression.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,upsample_thresh=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 regression model \n",
    "spi1_regression_model=initialize_regression_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_regression=spi1_regression_model.fit_generator(spi1_train_regression_gen,\n",
    "                                                  validation_data=spi1_valid_regression_gen,\n",
    "                                                  steps_per_epoch=1000000,\n",
    "                                                  validation_steps=100000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(history_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_regression_gen=DataGenerator(\"SPI1.test.regression.hdf5\",\n",
    "                                       \"hg19.genome.fa.gz\",\n",
    "                                         upsample=False,\n",
    "                                         add_revcomp=False,\n",
    "                                         batch_size=1000,\n",
    "                                         tasks=['SPI1'])\n",
    "spi1_test_regression_predictions=spi1_regression_model.predict_generator(spi1_test_regression_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "spi1_test_regression_truth=spi1_test_regression_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate spearman and pearson correlation between truth labels and predictions \n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "corr_pearson=pearsonr(spi1_test_regression_truth,spi1_test_regression_predictions)\n",
    "corr_spearman=spearmanr(spi1_test_regression_truth,spi1_test_regression_predictions)\n",
    "print(\"Pearson correlation on test set:\"+str(corr_pearson))\n",
    "print(\"Spearman correlation on test set:\"+str(corr_spearman))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on non-zero bins only  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.train.nonzero.regression.hdf5\",\n",
    "    'store_values_above_thresh':0,\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19','chrY'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':4,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(train_set_params)\n",
    "\n",
    "#2) Validation set: Chromosome 1\n",
    "valid_set_params={'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.valid.nonzero.regression.hdf5\",\n",
    "    'store_values_above_thresh':0,\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':1,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(valid_set_params)\n",
    "\n",
    "#3) Test set: Chromosomes 2, 19 \n",
    "test_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.test.nonzero.regression.hdf5\",\n",
    "    'store_values_above_thresh':0,\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':2,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(test_set_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_train_regression_gen=DataGenerator(\"SPI1.SPI1.train.nonzero.regression.hdf5\",\"hg19.genome.fa.gz\",upsample=False)\n",
    "spi1_valid_regression_gen=DataGenerator(\"SPI1.SPI1.valid.nonzero.regression.hdf5\",\"hg19.genome.fa.gz\",upsample=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 regression model \n",
    "spi1_regression_model=initialize_regression_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_regression=spi1_regression_model.fit_generator(spi1_train_regression_gen,\n",
    "                                                  validation_data=spi1_valid_regression_gen,\n",
    "                                                  epochs=150,\n",
    "                                                  steps_per_epoch=1000000,\n",
    "                                                  validation_steps=100000,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(history_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_regression_gen=DataGenerator(\"SPI1.test.regression.hdf5\",\n",
    "                                       \"hg19.genome.fa.gz\",\n",
    "                                         upsample=False,\n",
    "                                         add_revcomp=False,\n",
    "                                         batch_size=1000,\n",
    "                                         tasks=['SPI1'])\n",
    "spi1_test_regression_predictions=spi1_regression_model.predict_generator(spi1_test_regression_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "spi1_test_regression_truth=spi1_test_regression_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate spearman and pearson correlation between truth labels and predictions \n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "corr_pearson=pearsonr(spi1_test_regression_truth,spi1_test_regression_predictions)\n",
    "corr_spearman=spearmanr(spi1_test_regression_truth,spi1_test_regression_predictions)\n",
    "print(\"Pearson correlation on test set:\"+str(corr_pearson))\n",
    "print(\"Spearman correlation on test set:\"+str(corr_spearman))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT <a name='6'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the true positive predictions with a threshold of 0.9 (i.e. high confidence true positive predictions)\n",
    "spi1_test_classification_truth_bool=spi1_test_classification_truth.values.astype(bool)\n",
    "true_pos_spi1=spi1_test_classification_truth[spi1_test_classification_truth_bool*spi1_test_classification_predictions >0.9]\n",
    "true_pos_spi1.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_spi1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.utils import one_hot_from_bed\n",
    "deep_lift_input_spi1=one_hot_from_bed([i for i in true_pos_spi1.index],\"hg19.genome.fa.gz\")\n",
    "deep_lift_input_spi1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import deeplift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1=deeplift(spi1_classification_model,deep_lift_input_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a few of the DeepLIFT tracks and see if the model successfully learned SPI1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import  plot_seq_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[0],deep_lift_input_spi1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[1],deep_lift_input_spi1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2],deep_lift_input_spi1[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in to the center of one sequence so that it is easier to distinguish the motif: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2].squeeze()[400:500],deep_lift_input_spi1[2].squeeze()[400:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we query the sequence \"GTTTCACTTC\" in the [TomTom](http://meme-suite.org/tools/tomtom) software from the MEME suite, we find that the motif is a good match for IRF2, which is in the same motif family (Tryptophan cluster factors) as SPI1: \n",
    "<img src=\"tutorial_images/IRF.Tut4.png\" alt=\"IRF2TomTom\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering bQTL effect sizes: Classification vs Regression <a name='7'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.generators import * \n",
    "bqtl_ref_gen=BQTLGenerator(\"SPI1.bQTLs.txt.gz\",\"hg19.genome.fa.gz\",\"POSTallele\")\n",
    "bqtl_alt_gen=BQTLGenerator(\"SPI1.bQTLs.txt.gz\",\"hg19.genome.fa.gz\",\"ALTallele\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqtl_ref_classification_predictions=spi1_classification_model.predict_generator(bqtl_ref_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqtl_alt_classification_predictions=spi1_classification_model.predict_generator(bqtl_alt_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "bqtl_ref_classification_truth=bqtl_ref_gen.data['pvalue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bqtl_ref_classification_predictions.shape)\n",
    "print(bqtl_alt_classification_predictions.shape)\n",
    "print(bqtl_ref_classification_truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqtl_ref_regression_predictions=spi1_regression_model.predict_generator(bqtl_ref_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "bqtl_alt_regression_predictions=spi1_regression_model.predict_generator(bqtl_alt_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bqtl_ref_classification_predictions, bqtl_alt_classification_predictions, alpha=0.01)\n",
    "plt.xlabel(\"Ref\")\n",
    "plt.ylabel(\"Alt\")\n",
    "plt.title(\"BQTL Classification Model Predictions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bqtl_ref_regression_predictions, bqtl_alt_regression_predictions, alpha=0.01)\n",
    "plt.xlabel(\"Ref\")\n",
    "plt.ylabel(\"Alt\")\n",
    "plt.title(\"BQTL Regression Model Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-predicted SNP effect sizes vs bQTL effect sizes <a name='8'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logpval=np.log10(bqtl_ref_classification_truth.values)\n",
    "delta=bqtl_alt_classification_predictions-bqtl_ref_classification_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a name='9'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save tutorial outputs <a name='10'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "We save the models and test set predictions generated in this tutorial to an hdf5 file so that they can be loaded more readily in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the models \n",
    "spi1_classification_model.save(\"SPI1.classification.model.hdf5\")\n",
    "spi1_regression_model.save(\"SPI1.regression.model.hdf5\")\n",
    "#save the test predictions \n",
    "import h5py \n",
    "test_set_predictions=h5py.File(\"SPI1.test.predictions.hdf5\",'w')\n",
    "test_set_predictions.create_dataset(\"classification\",data=spi1_test_classification_predictions)\n",
    "test_set_predictions.create_dataset(\"regression\",data=spi1_test_regression_predictions)\n",
    "test_set_predictions.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
