{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train your DragoNN tutorial 4: \n",
    "## Interpreting predictive sequence features in in-vivo TF binding events\n",
    "\n",
    "This tutorial is a supplement to the DragoNN manuscript and follows figure 8 in the manuscript. \n",
    "\n",
    "This tutorial will take 2 - 3 hours if executed on a GPU.\n",
    "\n",
    "## Outline<a name='outline'>\n",
    "<ol>\n",
    "    <li><a href=#1>Input data</a></li>\n",
    "    <li><a href=#2>Generating positive and negative bins for genome-wide training </a></li>\n",
    "    <li><a href=#3>Challenges of in vivo data : batch generators and class upsampling </a></li>\n",
    "    <li><a href=#4>Case 1: Negatives consist of shuffled references, single-tasked models</a></li>  \n",
    "    <li><a href=#5>Case 2: Whole-genome negatives, single-tasked models </a></li>\n",
    "    <li><a href=#6>Case 3: Whole-genome negatives, multi-tasked models </a></li>\n",
    "    <li><a href=#7>Case 4: What happens if we don't upsample positive examples in our batches? </a></li>\n",
    "    <li><a href=#8>Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT </a></li>\n",
    "    <li><a href=#9>Conclusions</a></li>    \n",
    "</ol>\n",
    "Github issues on the dragonn repository with feedback, questions, and discussion are always welcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the lines below if you are running this tutorial from Google Colab \n",
    "#!pip install https://github.com/kundajelab/simdna/archive/0.3.zip\n",
    "#!pip install https://github.com/kundajelab/dragonn/archive/keras_2.2_tensorflow_1.6_purekeras.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure our results are reproducible\n",
    "from numpy.random import seed\n",
    "seed(1234)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/users/annashch/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#load dragonn tutorial utilities \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from dragonn.tutorial_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data <a name='1'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "Tutorials 1 - 3 have used simulated data generated with the simdna package. In this tutorial, we will examine how well CNN's are able to predict transcription factor binding for four TF's in vivo. \n",
    "\n",
    "We will learn to predict transcription factor binding for four transcription factors in the GM12878 cell line (one of the Tier 1 cell lines for the ENCODE project). First, we download the narrowPeak bed files for each of these transcription factors. You can skip the following code block if you already have the data downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CTCF, optimal IDR thresholded peaks, Stam Lab, hg19\n",
    "# https://www.encodeproject.org/experiments/ENCSR000DRZ/\n",
    "#!wget https://www.encodeproject.org/files/ENCFF473RXY/@@download/ENCFF473RXY.bed.gz \n",
    "\n",
    "## SPI1, optimal IDR thresholded peaks, Myers lab, hg19\n",
    "# https://www.encodeproject.org/experiments/ENCSR000BGQ/\n",
    "#!wget https://www.encodeproject.org/files/ENCFF002CHQ/@@download/ENCFF002CHQ.bed.gz\n",
    "    \n",
    "## ZNF143, optimal IDR thresholded peaks, Snyder lab, hg19\n",
    "#https://www.encodeproject.org/experiments/ENCSR936XTK/\n",
    "#!wget https://www.encodeproject.org/files/ENCFF544NXC/@@download/ENCFF544NXC.bed.gz\n",
    "\n",
    "## SIX5, optimal IDR thresholded peaks, Myers Lab, hg19\n",
    "# https://www.encodeproject.org/experiments/ENCSR000BJE/\n",
    "#!wget https://www.encodeproject.org/files/ENCFF606WUV/@@download/ENCFF606WUV.bed.gz\n",
    "\n",
    "## Download the hg19 chromsizes file (We only use chroms 1 -22, X, Y for training)\n",
    "#!wget https://github.com/kundajelab/dragonn/blob/keras_2.2_tensorflow_1.6_purekeras/paper_supplement/hg19.chrom.sizes\n",
    "    \n",
    "## Download the hg19 fasta reference genome (and corresponding .fai index)\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.fai \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating positive and negative bins for genome-wide training <a name='2'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the *genomewide_labels* function from the  [seqdataloader](https://github.com/kundajelab/seqdataloader) package to generate positive and negative labels for the TF-ChIPseq peaks across the genome. We will treat each sample as a task for the model and compare the performance of the model on SPI1 task in the single-tasked and multi-tasked setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqdataloader import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPI1\tENCFF002CHQ.bed.gz\t\r\n",
      "CTCF\tENCFF473RXY.bed.gz\t\r\n",
      "ZNF143\tENCFF544NXC.bed.gz\t\r\n",
      "SIX5\tENCFF606WUV.bed.gz\t\r\n"
     ]
    }
   ],
   "source": [
    "## seqdataloader accepts an input file, which we call tasks.tsv, with task names in column 1 and the corresponding\n",
    "## peak files in column 2 \n",
    "!cat tasks.tsv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the parameter configuration below, seqdataloader splits the genome into 1kb regions, with a stride of 50. Each 1kb region is centered at a 200 bp bin, with a left flank of 400 bases and a right flank of 400 bases. \n",
    "\n",
    "* Each 200 bp bin is labeled as positive if a narrowPeak summit overlaps with it. \n",
    "\n",
    "* The bin is labeled ambiguous (label = -1) and excluded from training if there is some overlap with the narrowPeak, but the peak summit does not lie in that overlap. \n",
    "\n",
    "* The bin is labeled negative if there is no overlap with the narrowPeak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary of bed files and bigwig files for each task:\n",
      "SPI1\n",
      "No BigWig file was provided for task:SPI1; Make sure this is intentional\n",
      "CTCF\n",
      "No BigWig file was provided for task:CTCF; Make sure this is intentional\n",
      "ZNF143\n",
      "No BigWig file was provided for task:ZNF143; Make sure this is intentional\n",
      "SIX5\n",
      "No BigWig file was provided for task:SIX5; Make sure this is intentional\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr9with dimensions:(2824249, 7)\n",
      "pre-allocated df for chrom:chr7with dimensions:(3182754, 7)\n",
      "got peak subset for chrom:chr7 for task:SIX5\n",
      "got peak subset for chrom:chr9 for task:SIX5\n",
      "got peak subset for chrom:chr9 for task:SPI1\n",
      "got peak subset for chrom:chr9 for task:CTCF\n",
      "got peak subset for chrom:chr7 for task:CTCF\n",
      "got peak subset for chrom:chr9 for task:ZNF143\n",
      "got peak subset for chrom:chr7 for task:SPI1\n",
      "finished chromosome:chr9 for task:SIX5\n",
      "pre-allocated df for chrom:chr5with dimensions:(3618286, 7)\n",
      "finished chromosome:chr7 for task:SIX5\n",
      "finished chromosome:chr9 for task:CTCF\n",
      "got peak subset for chrom:chr7 for task:ZNF143\n",
      "finished chromosome:chr7 for task:CTCF\n",
      "finished chromosome:chr9 for task:SPI1\n",
      "finished chromosome:chr9 for task:ZNF143\n",
      "finished chromosome:chr7 for task:SPI1\n",
      "finished chromosome:chr7 for task:ZNF143\n",
      "got peak subset for chrom:chr5 for task:SIX5\n",
      "got peak subset for chrom:chr5 for task:ZNF143\n",
      "finished chromosome:chr5 for task:SIX5finished chromosome:chr5 for task:ZNF143\n",
      "\n",
      "got peak subset for chrom:chr5 for task:CTCF\n",
      "finished chromosome:chr5 for task:CTCF\n",
      "got peak subset for chrom:chr5 for task:SPI1\n",
      "finished chromosome:chr5 for task:SPI1\n",
      "pre-allocated df for chrom:chr3with dimensions:(3960429, 7)\n",
      "got peak subset for chrom:chr3 for task:SPI1\n",
      "got peak subset for chrom:chr3 for task:ZNF143got peak subset for chrom:chr3 for task:SIX5finished chromosome:chr3 for task:SPI1\n",
      "\n",
      "\n",
      "got peak subset for chrom:chr3 for task:CTCF\n",
      "finished chromosome:chr3 for task:SIX5\n",
      "finished chromosome:chr3 for task:ZNF143\n",
      "finished chromosome:chr3 for task:CTCF\n",
      "pre-allocated df for chrom:chr10with dimensions:(2710675, 7)\n",
      "pre-allocated df for chrom:chr8with dimensions:(2927261, 7)\n",
      "got peak subset for chrom:chr10 for task:SIX5\n",
      "got peak subset for chrom:chr10 for task:ZNF143\n",
      "got peak subset for chrom:chr10 for task:CTCF\n",
      "got peak subset for chrom:chr8 for task:SIX5\n",
      "finished chromosome:chr10 for task:SIX5\n",
      "got peak subset for chrom:chr10 for task:SPI1\n",
      "finished chromosome:chr8 for task:SIX5got peak subset for chrom:chr8 for task:SPI1\n",
      "\n",
      "finished chromosome:chr10 for task:ZNF143\n",
      "got peak subset for chrom:chr8 for task:CTCF\n",
      "finished chromosome:chr10 for task:CTCF\n",
      "finished chromosome:chr8 for task:SPI1\n",
      "got peak subset for chrom:chr8 for task:ZNF143\n",
      "finished chromosome:chr8 for task:CTCF\n",
      "finished chromosome:chr10 for task:SPI1\n",
      "finished chromosome:chr8 for task:ZNF143\n",
      "pre-allocated df for chrom:chr4with dimensions:(3823066, 7)\n",
      "pre-allocated df for chrom:chr6with dimensions:(3422282, 7)\n",
      "got peak subset for chrom:chr4 for task:SPI1\n",
      "got peak subset for chrom:chr4 for task:ZNF143\n",
      "got peak subset for chrom:chr4 for task:CTCF\n",
      "got peak subset for chrom:chr6 for task:SPI1\n",
      "got peak subset for chrom:chr4 for task:SIX5\n",
      "got peak subset for chrom:chr6 for task:CTCF\n",
      "got peak subset for chrom:chr6 for task:SIX5\n",
      "finished chromosome:chr4 for task:SIX5\n",
      "finished chromosome:chr4 for task:ZNF143\n",
      "got peak subset for chrom:chr6 for task:ZNF143\n",
      "finished chromosome:chr4 for task:SPI1\n",
      "finished chromosome:chr6 for task:SIX5\n",
      "finished chromosome:chr4 for task:CTCF\n",
      "finished chromosome:chr6 for task:ZNF143\n",
      "finished chromosome:chr6 for task:SPI1\n",
      "finished chromosome:chr6 for task:CTCF\n",
      "pre-allocated df for chrom:chr13with dimensions:(2303378, 7)\n",
      "pre-allocated df for chrom:chr11with dimensions:(2700111, 7)\n",
      "got peak subset for chrom:chr13 for task:SIX5\n",
      "got peak subset for chrom:chr13 for task:SPI1\n",
      "finished chromosome:chr13 for task:SIX5\n",
      "got peak subset for chrom:chr13 for task:CTCF\n",
      "got peak subset for chrom:chr13 for task:ZNF143\n",
      "got peak subset for chrom:chr11 for task:SPI1\n",
      "got peak subset for chrom:chr11 for task:CTCF\n",
      "got peak subset for chrom:chr11 for task:SIX5\n",
      "got peak subset for chrom:chr11 for task:ZNF143\n",
      "finished chromosome:chr13 for task:CTCF\n",
      "finished chromosome:chr13 for task:ZNF143\n",
      "finished chromosome:chr11 for task:SIX5\n",
      "finished chromosome:chr13 for task:SPI1\n",
      "finished chromosome:chr11 for task:ZNF143\n",
      "finished chromosome:chr11 for task:CTCF\n",
      "finished chromosome:chr11 for task:SPI1\n",
      "pre-allocated df for chrom:chr17with dimensions:(1623885, 7)\n",
      "got peak subset for chrom:chr17 for task:SPI1\n",
      "got peak subset for chrom:chr17 for task:ZNF143\n",
      "got peak subset for chrom:chr17 for task:CTCF\n",
      "finished chromosome:chr17 for task:SPI1\n",
      "finished chromosome:chr17 for task:ZNF143got peak subset for chrom:chr17 for task:SIX5\n",
      "\n",
      "finished chromosome:chr17 for task:CTCF\n",
      "finished chromosome:chr17 for task:SIX5\n",
      "pre-allocated df for chrom:chr15with dimensions:(2050608, 7)\n",
      "got peak subset for chrom:chr15 for task:SIX5\n",
      "got peak subset for chrom:chr15 for task:SPI1\n",
      "got peak subset for chrom:chr15 for task:ZNF143\n",
      "finished chromosome:chr15 for task:SIX5\n",
      "finished chromosome:chr15 for task:ZNF143\n",
      "got peak subset for chrom:chr15 for task:CTCF\n",
      "finished chromosome:chr15 for task:SPI1\n",
      "finished chromosome:chr15 for task:CTCF\n",
      "pre-allocated df for chrom:chr14with dimensions:(2146971, 7)\n",
      "pre-allocated df for chrom:chr18with dimensions:(1561525, 7)\n",
      "got peak subset for chrom:chr14 for task:CTCF\n",
      "got peak subset for chrom:chr14 for task:ZNF143\n",
      "got peak subset for chrom:chr14 for task:SIX5\n",
      "got peak subset for chrom:chr14 for task:SPI1\n",
      "finished chromosome:chr14 for task:ZNF143\n",
      "finished chromosome:chr14 for task:SIX5\n",
      "got peak subset for chrom:chr18 for task:SPI1\n",
      "got peak subset for chrom:chr18 for task:ZNF143\n",
      "got peak subset for chrom:chr18 for task:SIX5\n",
      "finished chromosome:chr14 for task:SPI1\n",
      "finished chromosome:chr14 for task:CTCF\n",
      "got peak subset for chrom:chr18 for task:CTCF\n",
      "finished chromosome:chr18 for task:ZNF143\n",
      "pre-allocated df for chrom:chr12with dimensions:(2677018, 7)\n",
      "finished chromosome:chr18 for task:SPI1\n",
      "finished chromosome:chr18 for task:SIX5\n",
      "finished chromosome:chr18 for task:CTCF\n",
      "got peak subset for chrom:chr12 for task:ZNF143\n",
      "got peak subset for chrom:chr12 for task:CTCF\n",
      "got peak subset for chrom:chr12 for task:SIX5\n",
      "got peak subset for chrom:chr12 for task:SPI1\n",
      "finished chromosome:chr12 for task:SIX5\n",
      "finished chromosome:chr12 for task:ZNF143\n",
      "finished chromosome:chr12 for task:CTCF\n",
      "finished chromosome:chr12 for task:SPI1\n",
      "pre-allocated df for chrom:chr16with dimensions:(1807076, 7)\n",
      "got peak subset for chrom:chr16 for task:ZNF143\n",
      "got peak subset for chrom:chr16 for task:CTCF\n",
      "finished chromosome:chr16 for task:ZNF143\n",
      "got peak subset for chrom:chr16 for task:SIX5\n",
      "finished chromosome:chr16 for task:CTCF\n",
      "got peak subset for chrom:chr16 for task:SPI1\n",
      "finished chromosome:chr16 for task:SIX5\n",
      "finished chromosome:chr16 for task:SPI1\n",
      "pre-allocated df for chrom:chr22with dimensions:(1026072, 7)\n",
      "got peak subset for chrom:chr22 for task:ZNF143\n",
      "got peak subset for chrom:chr22 for task:SIX5\n",
      "pre-allocated df for chrom:chr20with dimensions:(1260491, 7)\n",
      "got peak subset for chrom:chr22 for task:CTCF\n",
      "finished chromosome:chr22 for task:ZNF143\n",
      "finished chromosome:chr22 for task:SIX5\n",
      "got peak subset for chrom:chr22 for task:SPI1\n",
      "finished chromosome:chr22 for task:CTCF\n",
      "finished chromosome:chr22 for task:SPI1\n",
      "got peak subset for chrom:chr20 for task:CTCF\n",
      "got peak subset for chrom:chr20 for task:ZNF143\n",
      "got peak subset for chrom:chr20 for task:SIX5\n",
      "finished chromosome:chr20 for task:ZNF143\n",
      "finished chromosome:chr20 for task:CTCF\n",
      "finished chromosome:chr20 for task:SIX5\n",
      "got peak subset for chrom:chr20 for task:SPI1\n",
      "pre-allocated df for chrom:chrYwith dimensions:(1187452, 7)\n",
      "finished chromosome:chr20 for task:SPI1\n",
      "got peak subset for chrom:chrY for task:SPI1\n",
      "got peak subset for chrom:chrY for task:SIX5got peak subset for chrom:chrY for task:ZNF143\n",
      "\n",
      "finished chromosome:chrY for task:SPI1\n",
      "finished chromosome:chrY for task:ZNF143\n",
      "finished chromosome:chrY for task:SIX5\n",
      "got peak subset for chrom:chrY for task:CTCF\n",
      "finished chromosome:chrY for task:CTCF\n",
      "pre-allocated df for chrom:chr21with dimensions:(962578, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got peak subset for chrom:chr21 for task:SPI1\n",
      "got peak subset for chrom:chr21 for task:ZNF143\n",
      "got peak subset for chrom:chr21 for task:CTCF\n",
      "got peak subset for chrom:chr21 for task:SIX5\n",
      "finished chromosome:chr21 for task:ZNF143\n",
      "finished chromosome:chr21 for task:SPI1\n",
      "finished chromosome:chr21 for task:SIX5\n",
      "finished chromosome:chr21 for task:CTCF\n",
      "pre-allocated df for chrom:chrXwith dimensions:(3105392, 7)\n",
      "got peak subset for chrom:chrX for task:SIX5\n",
      "got peak subset for chrom:chrX for task:CTCF\n",
      "finished chromosome:chrX for task:CTCF\n",
      "finished chromosome:chrX for task:SIX5\n",
      "got peak subset for chrom:chrX for task:ZNF143\n",
      "finished chromosome:chrX for task:ZNF143\n",
      "got peak subset for chrom:chrX for task:SPI1\n",
      "finished chromosome:chrX for task:SPI1\n",
      "expanding chromosome pool outputs\n",
      "concatenating data frames for chromosomes\n",
      "writing output dataframe to disk\n",
      "done!\n",
      "creating dictionary of bed files and bigwig files for each task:\n",
      "SPI1\n",
      "No BigWig file was provided for task:SPI1; Make sure this is intentional\n",
      "CTCF\n",
      "No BigWig file was provided for task:CTCF; Make sure this is intentional\n",
      "ZNF143\n",
      "No BigWig file was provided for task:ZNF143; Make sure this is intentional\n",
      "SIX5\n",
      "No BigWig file was provided for task:SIX5; Make sure this is intentional\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr7with dimensions:(3182754, 7)\n",
      "got peak subset for chrom:chr7 for task:ZNF143\n",
      "got peak subset for chrom:chr7 for task:SPI1\n",
      "pre-allocated df for chrom:chr9with dimensions:(2824249, 7)\n",
      "finished chromosome:chr7 for task:ZNF143\n",
      "got peak subset for chrom:chr7 for task:SIX5\n",
      "got peak subset for chrom:chr7 for task:CTCF\n",
      "finished chromosome:chr7 for task:SPI1\n",
      "finished chromosome:chr7 for task:SIX5\n",
      "got peak subset for chrom:chr9 for task:ZNF143\n",
      "finished chromosome:chr7 for task:CTCF\n",
      "got peak subset for chrom:chr9 for task:SPI1\n",
      "got peak subset for chrom:chr9 for task:SIX5\n",
      "finished chromosome:chr9 for task:ZNF143\n",
      "finished chromosome:chr9 for task:SIX5\n",
      "finished chromosome:chr9 for task:SPI1\n",
      "pre-allocated df for chrom:chr3with dimensions:(3960429, 7)\n",
      "got peak subset for chrom:chr9 for task:CTCF\n",
      "got peak subset for chrom:chr3 for task:ZNF143\n",
      "got peak subset for chrom:chr3 for task:CTCF\n",
      "finished chromosome:chr9 for task:CTCF\n",
      "got peak subset for chrom:chr3 for task:SIX5\n",
      "finished chromosome:chr3 for task:ZNF143\n",
      "got peak subset for chrom:chr3 for task:SPI1\n",
      "finished chromosome:chr3 for task:CTCF\n",
      "pre-allocated df for chrom:chr5with dimensions:(3618286, 7)\n",
      "finished chromosome:chr3 for task:SIX5\n",
      "finished chromosome:chr3 for task:SPI1\n",
      "got peak subset for chrom:chr5 for task:CTCF\n",
      "got peak subset for chrom:chr5 for task:SIX5\n",
      "got peak subset for chrom:chr5 for task:ZNF143\n",
      "finished chromosome:chr5 for task:SIX5\n",
      "finished chromosome:chr5 for task:CTCF\n",
      "finished chromosome:chr5 for task:ZNF143\n",
      "got peak subset for chrom:chr5 for task:SPI1\n",
      "finished chromosome:chr5 for task:SPI1\n",
      "pre-allocated df for chrom:chr10with dimensions:(2710675, 7)\n",
      "pre-allocated df for chrom:chr8with dimensions:(2927261, 7)got peak subset for chrom:chr10 for task:CTCF\n",
      "\n",
      "got peak subset for chrom:chr10 for task:ZNF143\n",
      "got peak subset for chrom:chr10 for task:SIX5\n",
      "got peak subset for chrom:chr10 for task:SPI1\n",
      "finished chromosome:chr10 for task:SIX5\n",
      "finished chromosome:chr10 for task:CTCF\n",
      "finished chromosome:chr10 for task:ZNF143\n",
      "finished chromosome:chr10 for task:SPI1\n",
      "pre-allocated df for chrom:chr4with dimensions:(3823066, 7)\n",
      "got peak subset for chrom:chr8 for task:ZNF143\n",
      "got peak subset for chrom:chr8 for task:CTCF\n",
      "got peak subset for chrom:chr8 for task:SPI1\n",
      "got peak subset for chrom:chr8 for task:SIX5\n",
      "got peak subset for chrom:chr4 for task:ZNF143\n",
      "got peak subset for chrom:chr4 for task:SIX5\n",
      "finished chromosome:chr8 for task:ZNF143\n",
      "finished chromosome:chr4 for task:SIX5\n",
      "finished chromosome:chr8 for task:SIX5\n",
      "finished chromosome:chr4 for task:ZNF143\n",
      "got peak subset for chrom:chr4 for task:CTCF\n",
      "got peak subset for chrom:chr4 for task:SPI1\n",
      "finished chromosome:chr8 for task:CTCF\n",
      "finished chromosome:chr8 for task:SPI1\n",
      "pre-allocated df for chrom:chr6with dimensions:(3422282, 7)\n",
      "finished chromosome:chr4 for task:SPI1\n",
      "finished chromosome:chr4 for task:CTCF\n",
      "got peak subset for chrom:chr6 for task:ZNF143got peak subset for chrom:chr6 for task:SPI1\n",
      "\n",
      "got peak subset for chrom:chr6 for task:CTCF\n",
      "finished chromosome:chr6 for task:ZNF143\n",
      "got peak subset for chrom:chr6 for task:SIX5\n",
      "finished chromosome:chr6 for task:SIX5\n",
      "finished chromosome:chr6 for task:CTCF\n",
      "finished chromosome:chr6 for task:SPI1\n",
      "pre-allocated df for chrom:chr15with dimensions:(2050608, 7)\n",
      "pre-allocated df for chrom:chr13with dimensions:(2303378, 7)\n",
      "got peak subset for chrom:chr15 for task:ZNF143\n",
      "got peak subset for chrom:chr15 for task:SIX5\n",
      "got peak subset for chrom:chr15 for task:CTCF\n",
      "pre-allocated df for chrom:chr17with dimensions:(1623885, 7)\n",
      "got peak subset for chrom:chr15 for task:SPI1\n",
      "finished chromosome:chr15 for task:SIX5\n",
      "finished chromosome:chr15 for task:ZNF143\n",
      "finished chromosome:chr15 for task:CTCF\n",
      "finished chromosome:chr15 for task:SPI1got peak subset for chrom:chr13 for task:SIX5\n",
      "got peak subset for chrom:chr13 for task:ZNF143\n",
      "\n",
      "got peak subset for chrom:chr13 for task:CTCF\n",
      "got peak subset for chrom:chr13 for task:SPI1\n",
      "got peak subset for chrom:chr17 for task:CTCF\n",
      "finished chromosome:chr13 for task:SIX5\n",
      "got peak subset for chrom:chr17 for task:SPI1\n",
      "finished chromosome:chr13 for task:ZNF143\n",
      "finished chromosome:chr13 for task:SPI1got peak subset for chrom:chr17 for task:ZNF143\n",
      "\n",
      "finished chromosome:chr13 for task:CTCF\n",
      "got peak subset for chrom:chr17 for task:SIX5\n",
      "pre-allocated df for chrom:chr11with dimensions:(2700111, 7)\n",
      "finished chromosome:chr17 for task:CTCF\n",
      "finished chromosome:chr17 for task:SIX5\n",
      "finished chromosome:chr17 for task:SPI1\n",
      "finished chromosome:chr17 for task:ZNF143\n",
      "got peak subset for chrom:chr11 for task:SIX5\n",
      "got peak subset for chrom:chr11 for task:ZNF143\n",
      "got peak subset for chrom:chr11 for task:SPI1\n",
      "finished chromosome:chr11 for task:SIX5\n",
      "got peak subset for chrom:chr11 for task:CTCF\n",
      "finished chromosome:chr11 for task:ZNF143\n",
      "finished chromosome:chr11 for task:CTCF\n",
      "finished chromosome:chr11 for task:SPI1\n",
      "pre-allocated df for chrom:chr16with dimensions:(1807076, 7)\n",
      "pre-allocated df for chrom:chr18with dimensions:(1561525, 7)got peak subset for chrom:chr16 for task:ZNF143\n",
      "\n",
      "got peak subset for chrom:chr16 for task:SIX5\n",
      "got peak subset for chrom:chr16 for task:SPI1\n",
      "got peak subset for chrom:chr16 for task:CTCF\n",
      "finished chromosome:chr16 for task:SIX5\n",
      "finished chromosome:chr16 for task:ZNF143\n",
      "finished chromosome:chr16 for task:SPI1\n",
      "got peak subset for chrom:chr18 for task:SPI1\n",
      "finished chromosome:chr16 for task:CTCFgot peak subset for chrom:chr18 for task:SIX5\n",
      "\n",
      "got peak subset for chrom:chr18 for task:CTCF\n",
      "finished chromosome:chr18 for task:SIX5\n",
      "got peak subset for chrom:chr18 for task:ZNF143\n",
      "finished chromosome:chr18 for task:SPI1\n",
      "finished chromosome:chr18 for task:ZNF143\n",
      "finished chromosome:chr18 for task:CTCF\n",
      "pre-allocated df for chrom:chr14with dimensions:(2146971, 7)\n",
      "got peak subset for chrom:chr14 for task:SPI1\n",
      "got peak subset for chrom:chr14 for task:ZNF143\n",
      "got peak subset for chrom:chr14 for task:CTCF\n",
      "got peak subset for chrom:chr14 for task:SIX5\n",
      "finished chromosome:chr14 for task:SPI1\n",
      "finished chromosome:chr14 for task:ZNF143\n",
      "finished chromosome:chr14 for task:CTCF\n",
      "finished chromosome:chr14 for task:SIX5\n",
      "pre-allocated df for chrom:chr12with dimensions:(2677018, 7)\n",
      "pre-allocated df for chrom:chr22with dimensions:(1026072, 7)\n",
      "got peak subset for chrom:chr12 for task:SPI1\n",
      "got peak subset for chrom:chr12 for task:ZNF143\n",
      "got peak subset for chrom:chr12 for task:CTCF\n",
      "got peak subset for chrom:chr12 for task:SIX5\n",
      "pre-allocated df for chrom:chr20with dimensions:(1260491, 7)\n",
      "got peak subset for chrom:chr22 for task:CTCF\n",
      "got peak subset for chrom:chr22 for task:SIX5\n",
      "finished chromosome:chr12 for task:SPI1\n",
      "finished chromosome:chr12 for task:SIX5finished chromosome:chr12 for task:CTCF\n",
      "finished chromosome:chr22 for task:SIX5\n",
      "\n",
      "finished chromosome:chr12 for task:ZNF143\n",
      "got peak subset for chrom:chr22 for task:ZNF143\n",
      "finished chromosome:chr22 for task:CTCF\n",
      "got peak subset for chrom:chr22 for task:SPI1\n",
      "finished chromosome:chr22 for task:ZNF143\n",
      "finished chromosome:chr22 for task:SPI1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got peak subset for chrom:chr20 for task:SPI1\n",
      "got peak subset for chrom:chr20 for task:ZNF143\n",
      "got peak subset for chrom:chr20 for task:CTCF\n",
      "got peak subset for chrom:chr20 for task:SIX5\n",
      "finished chromosome:chr20 for task:SPI1\n",
      "finished chromosome:chr20 for task:CTCF\n",
      "finished chromosome:chr20 for task:SIX5\n",
      "finished chromosome:chr20 for task:ZNF143\n",
      "pre-allocated df for chrom:chrYwith dimensions:(1187452, 7)\n",
      "got peak subset for chrom:chrY for task:SPI1\n",
      "got peak subset for chrom:chrY for task:ZNF143\n",
      "finished chromosome:chrY for task:SPI1\n",
      "got peak subset for chrom:chrY for task:CTCF\n",
      "got peak subset for chrom:chrY for task:SIX5\n",
      "finished chromosome:chrY for task:ZNF143\n",
      "finished chromosome:chrY for task:CTCF\n",
      "finished chromosome:chrY for task:SIX5\n",
      "pre-allocated df for chrom:chr21with dimensions:(962578, 7)\n",
      "got peak subset for chrom:chr21 for task:CTCFgot peak subset for chrom:chr21 for task:SPI1\n",
      "\n",
      "got peak subset for chrom:chr21 for task:ZNF143\n",
      "finished chromosome:chr21 for task:SPI1\n",
      "finished chromosome:chr21 for task:CTCF\n",
      "got peak subset for chrom:chr21 for task:SIX5\n",
      "finished chromosome:chr21 for task:ZNF143\n",
      "finished chromosome:chr21 for task:SIX5\n",
      "pre-allocated df for chrom:chrXwith dimensions:(3105392, 7)\n",
      "got peak subset for chrom:chrX for task:SIX5got peak subset for chrom:chrX for task:ZNF143\n",
      "\n",
      "finished chromosome:chrX for task:SIX5\n",
      "finished chromosome:chrX for task:ZNF143\n",
      "got peak subset for chrom:chrX for task:CTCF\n",
      "finished chromosome:chrX for task:CTCF\n",
      "got peak subset for chrom:chrX for task:SPI1\n",
      "finished chromosome:chrX for task:SPI1\n",
      "expanding chromosome pool outputs\n",
      "concatenating data frames for chromosomes\n",
      "writing output dataframe to disk\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#we will include all chromosomes with the exception of 1,2, and 19 in our training set \n",
    "\n",
    "#1) Generate genome-wide negatives in addition to positives \n",
    "train_set_params={\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"TF.train.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':4,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(train_set_params)\n",
    "\n",
    "#2) Extract positive bins for each task for DeepBind training paradigm -- shuffled reference negatives to be \n",
    "#generated on the fly \n",
    "\n",
    "positives_train_set_params={\n",
    "    'store_positives_only':True,\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"positives.TF.train.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':4,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(positives_train_set_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary of bed files and bigwig files for each task:\n",
      "SPI1\n",
      "No BigWig file was provided for task:SPI1; Make sure this is intentional\n",
      "CTCF\n",
      "No BigWig file was provided for task:CTCF; Make sure this is intentional\n",
      "ZNF143\n",
      "No BigWig file was provided for task:ZNF143; Make sure this is intentional\n",
      "SIX5\n",
      "No BigWig file was provided for task:SIX5; Make sure this is intentional\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr1with dimensions:(4984993, 7)\n",
      "got peak subset for chrom:chr1 for task:SPI1\n",
      "finished chromosome:chr1 for task:SPI1\n",
      "got peak subset for chrom:chr1 for task:CTCF\n",
      "finished chromosome:chr1 for task:CTCF\n",
      "got peak subset for chrom:chr1 for task:ZNF143\n",
      "finished chromosome:chr1 for task:ZNF143\n",
      "got peak subset for chrom:chr1 for task:SIX5\n",
      "finished chromosome:chr1 for task:SIX5\n",
      "expanding chromosome pool outputs\n",
      "concatenating data frames for chromosomes\n",
      "writing output dataframe to disk\n",
      "done!\n",
      "creating dictionary of bed files and bigwig files for each task:\n",
      "SPI1\n",
      "No BigWig file was provided for task:SPI1; Make sure this is intentional\n",
      "CTCF\n",
      "No BigWig file was provided for task:CTCF; Make sure this is intentional\n",
      "ZNF143\n",
      "No BigWig file was provided for task:ZNF143; Make sure this is intentional\n",
      "SIX5\n",
      "No BigWig file was provided for task:SIX5; Make sure this is intentional\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr1with dimensions:(4984993, 7)\n",
      "got peak subset for chrom:chr1 for task:SPI1\n",
      "finished chromosome:chr1 for task:SPI1\n",
      "got peak subset for chrom:chr1 for task:CTCF\n",
      "finished chromosome:chr1 for task:CTCF\n",
      "got peak subset for chrom:chr1 for task:ZNF143\n",
      "finished chromosome:chr1 for task:ZNF143\n",
      "got peak subset for chrom:chr1 for task:SIX5\n",
      "finished chromosome:chr1 for task:SIX5\n",
      "expanding chromosome pool outputs\n",
      "concatenating data frames for chromosomes\n",
      "writing output dataframe to disk\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#We will include chromsoome 1 in our validation set \n",
    "\n",
    "#1) Generate genome-wide negatives in addition to positives \n",
    "valid_set_params={'task_list':\"tasks.tsv\",\n",
    "    'outf':\"TF.valid.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':1,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(valid_set_params)\n",
    "\n",
    "\n",
    "#2) Extract positive bins for each task for DeepBind training paradigm -- shuffled reference negatives to be \n",
    "#generated on the fly \n",
    "positives_valid_set_params={\n",
    "    'store_positives_only':True,\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"positives.TF.valid.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':1,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(positives_valid_set_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary of bed files and bigwig files for each task:\n",
      "SPI1\n",
      "No BigWig file was provided for task:SPI1; Make sure this is intentional\n",
      "CTCF\n",
      "No BigWig file was provided for task:CTCF; Make sure this is intentional\n",
      "ZNF143\n",
      "No BigWig file was provided for task:ZNF143; Make sure this is intentional\n",
      "SIX5\n",
      "No BigWig file was provided for task:SIX5; Make sure this is intentional\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr19with dimensions:(1182560, 7)\n",
      "got peak subset for chrom:chr19 for task:SPI1\n",
      "got peak subset for chrom:chr19 for task:CTCF\n",
      "finished chromosome:chr19 for task:SPI1\n",
      "finished chromosome:chr19 for task:CTCF\n",
      "got peak subset for chrom:chr19 for task:ZNF143\n",
      "got peak subset for chrom:chr19 for task:SIX5\n",
      "finished chromosome:chr19 for task:ZNF143\n",
      "finished chromosome:chr19 for task:SIX5\n",
      "pre-allocated df for chrom:chr2with dimensions:(4863968, 7)\n",
      "got peak subset for chrom:chr2 for task:CTCF\n",
      "finished chromosome:chr2 for task:CTCF\n",
      "got peak subset for chrom:chr2 for task:SPI1\n",
      "finished chromosome:chr2 for task:SPI1\n",
      "got peak subset for chrom:chr2 for task:ZNF143\n",
      "finished chromosome:chr2 for task:ZNF143\n",
      "got peak subset for chrom:chr2 for task:SIX5\n",
      "finished chromosome:chr2 for task:SIX5\n",
      "expanding chromosome pool outputs\n",
      "concatenating data frames for chromosomes\n",
      "writing output dataframe to disk\n",
      "done!\n",
      "creating dictionary of bed files and bigwig files for each task:\n",
      "SPI1\n",
      "No BigWig file was provided for task:SPI1; Make sure this is intentional\n",
      "CTCF\n",
      "No BigWig file was provided for task:CTCF; Make sure this is intentional\n",
      "ZNF143\n",
      "No BigWig file was provided for task:ZNF143; Make sure this is intentional\n",
      "SIX5\n",
      "No BigWig file was provided for task:SIX5; Make sure this is intentional\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr19with dimensions:(1182560, 7)\n",
      "got peak subset for chrom:chr19 for task:SPI1\n",
      "got peak subset for chrom:chr19 for task:CTCF\n",
      "finished chromosome:chr19 for task:SPI1\n",
      "finished chromosome:chr19 for task:CTCF\n",
      "got peak subset for chrom:chr19 for task:SIX5got peak subset for chrom:chr19 for task:ZNF143\n",
      "\n",
      "finished chromosome:chr19 for task:SIX5\n",
      "finished chromosome:chr19 for task:ZNF143\n",
      "pre-allocated df for chrom:chr2with dimensions:(4863968, 7)\n",
      "got peak subset for chrom:chr2 for task:CTCFgot peak subset for chrom:chr2 for task:SPI1\n",
      "\n",
      "finished chromosome:chr2 for task:CTCF\n",
      "finished chromosome:chr2 for task:SPI1\n",
      "got peak subset for chrom:chr2 for task:ZNF143\n",
      "got peak subset for chrom:chr2 for task:SIX5\n",
      "finished chromosome:chr2 for task:ZNF143\n",
      "finished chromosome:chr2 for task:SIX5\n",
      "expanding chromosome pool outputs\n",
      "concatenating data frames for chromosomes\n",
      "writing output dataframe to disk\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#We will include chromosomes 2 and 19 in our testing set \n",
    "test_set_params={\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"TF.test.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':2,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(test_set_params)\n",
    "\n",
    "#2) Extract positive bins for each task for DeepBind training paradigm -- shuffled reference negatives to be \n",
    "#generated on the fly \n",
    "positives_test_set_params={\n",
    "    'store_positives_only':True,\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"positives.TF.test.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':2,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':False,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(positives_test_set_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the files that were generated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHR</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th>SPI1</th>\n",
       "      <th>CTCF</th>\n",
       "      <th>ZNF143</th>\n",
       "      <th>SIX5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr3</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr3</td>\n",
       "      <td>50</td>\n",
       "      <td>1050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr3</td>\n",
       "      <td>100</td>\n",
       "      <td>1100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr3</td>\n",
       "      <td>150</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr3</td>\n",
       "      <td>200</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chr3</td>\n",
       "      <td>250</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chr3</td>\n",
       "      <td>300</td>\n",
       "      <td>1300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chr3</td>\n",
       "      <td>350</td>\n",
       "      <td>1350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chr3</td>\n",
       "      <td>400</td>\n",
       "      <td>1400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chr3</td>\n",
       "      <td>450</td>\n",
       "      <td>1450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CHR  START   END  SPI1  CTCF  ZNF143  SIX5\n",
       "0  chr3      0  1000   0.0   0.0     0.0   0.0\n",
       "1  chr3     50  1050   0.0   0.0     0.0   0.0\n",
       "2  chr3    100  1100   0.0   0.0     0.0   0.0\n",
       "3  chr3    150  1150   0.0   0.0     0.0   0.0\n",
       "4  chr3    200  1200   0.0   0.0     0.0   0.0\n",
       "5  chr3    250  1250   0.0   0.0     0.0   0.0\n",
       "6  chr3    300  1300   0.0   0.0     0.0   0.0\n",
       "7  chr3    350  1350   0.0   0.0     0.0   0.0\n",
       "8  chr3    400  1400   0.0   0.0     0.0   0.0\n",
       "9  chr3    450  1450   0.0   0.0     0.0   0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The code generates bed file outputs with a label of 1 or 0 for each 1kb\n",
    "# genome bin for each task. Note that the bins are shifted with a stride of 50.\n",
    "pd.read_hdf(\"TF.train.hdf5\",start=0,stop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHR</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th>SPI1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>chr3</td>\n",
       "      <td>260350</td>\n",
       "      <td>261350</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>chr3</td>\n",
       "      <td>260400</td>\n",
       "      <td>261400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5209</th>\n",
       "      <td>chr3</td>\n",
       "      <td>260450</td>\n",
       "      <td>261450</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>chr3</td>\n",
       "      <td>260500</td>\n",
       "      <td>261500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6383</th>\n",
       "      <td>chr3</td>\n",
       "      <td>319150</td>\n",
       "      <td>320150</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6384</th>\n",
       "      <td>chr3</td>\n",
       "      <td>319200</td>\n",
       "      <td>320200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6385</th>\n",
       "      <td>chr3</td>\n",
       "      <td>319250</td>\n",
       "      <td>320250</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6386</th>\n",
       "      <td>chr3</td>\n",
       "      <td>319300</td>\n",
       "      <td>320300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6401</th>\n",
       "      <td>chr3</td>\n",
       "      <td>320050</td>\n",
       "      <td>321050</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6402</th>\n",
       "      <td>chr3</td>\n",
       "      <td>320100</td>\n",
       "      <td>321100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CHR   START     END  SPI1\n",
       "5207  chr3  260350  261350   1.0\n",
       "5208  chr3  260400  261400   1.0\n",
       "5209  chr3  260450  261450   1.0\n",
       "5210  chr3  260500  261500   1.0\n",
       "6383  chr3  319150  320150   1.0\n",
       "6384  chr3  319200  320200   1.0\n",
       "6385  chr3  319250  320250   1.0\n",
       "6386  chr3  319300  320300   1.0\n",
       "6401  chr3  320050  321050   1.0\n",
       "6402  chr3  320100  321100   1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When provided with the --store-positives_only flag, the code generates all bins for each task that are labeled positive.\n",
    "pd.read_hdf(\"SPI1.positives.TF.train.hdf5\",start=0,stop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>SPI1</th>\n",
       "      <th>CTCF</th>\n",
       "      <th>ZNF143</th>\n",
       "      <th>SIX5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHR</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">chr2</th>\n",
       "      <th>0</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <th>1050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <th>1100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <th>1150</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <th>1200</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <th>1250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <th>1300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <th>1350</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <th>1400</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <th>1450</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <th>1500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <th>1550</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <th>1600</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <th>1650</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <th>1700</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <th>1750</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <th>1800</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <th>1850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <th>1900</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <th>1950</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <th>2000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <th>2050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <th>2100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <th>2150</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <th>2200</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <th>2250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <th>2300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <th>2350</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <th>2400</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <th>2450</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">chr19</th>\n",
       "      <th>59126500</th>\n",
       "      <th>59127500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126550</th>\n",
       "      <th>59127550</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126600</th>\n",
       "      <th>59127600</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126650</th>\n",
       "      <th>59127650</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126700</th>\n",
       "      <th>59127700</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126750</th>\n",
       "      <th>59127750</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126800</th>\n",
       "      <th>59127800</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126850</th>\n",
       "      <th>59127850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126900</th>\n",
       "      <th>59127900</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126950</th>\n",
       "      <th>59127950</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127000</th>\n",
       "      <th>59128000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127050</th>\n",
       "      <th>59128050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127100</th>\n",
       "      <th>59128100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127150</th>\n",
       "      <th>59128150</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127200</th>\n",
       "      <th>59128200</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127250</th>\n",
       "      <th>59128250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127300</th>\n",
       "      <th>59128300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127350</th>\n",
       "      <th>59128350</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127400</th>\n",
       "      <th>59128400</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127450</th>\n",
       "      <th>59128450</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127500</th>\n",
       "      <th>59128500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127550</th>\n",
       "      <th>59128550</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127600</th>\n",
       "      <th>59128600</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127650</th>\n",
       "      <th>59128650</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127700</th>\n",
       "      <th>59128700</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127750</th>\n",
       "      <th>59128750</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127800</th>\n",
       "      <th>59128800</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127850</th>\n",
       "      <th>59128850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127900</th>\n",
       "      <th>59128900</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127950</th>\n",
       "      <th>59128950</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6046528 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         SPI1  CTCF  ZNF143  SIX5\n",
       "CHR   START    END                               \n",
       "chr2  0        1000       0.0   0.0     0.0   0.0\n",
       "      50       1050       0.0   0.0     0.0   0.0\n",
       "      100      1100       0.0   0.0     0.0   0.0\n",
       "      150      1150       0.0   0.0     0.0   0.0\n",
       "      200      1200       0.0   0.0     0.0   0.0\n",
       "      250      1250       0.0   0.0     0.0   0.0\n",
       "      300      1300       0.0   0.0     0.0   0.0\n",
       "      350      1350       0.0   0.0     0.0   0.0\n",
       "      400      1400       0.0   0.0     0.0   0.0\n",
       "      450      1450       0.0   0.0     0.0   0.0\n",
       "      500      1500       0.0   0.0     0.0   0.0\n",
       "      550      1550       0.0   0.0     0.0   0.0\n",
       "      600      1600       0.0   0.0     0.0   0.0\n",
       "      650      1650       0.0   0.0     0.0   0.0\n",
       "      700      1700       0.0   0.0     0.0   0.0\n",
       "      750      1750       0.0   0.0     0.0   0.0\n",
       "      800      1800       0.0   0.0     0.0   0.0\n",
       "      850      1850       0.0   0.0     0.0   0.0\n",
       "      900      1900       0.0   0.0     0.0   0.0\n",
       "      950      1950       0.0   0.0     0.0   0.0\n",
       "      1000     2000       0.0   0.0     0.0   0.0\n",
       "      1050     2050       0.0   0.0     0.0   0.0\n",
       "      1100     2100       0.0   0.0     0.0   0.0\n",
       "      1150     2150       0.0   0.0     0.0   0.0\n",
       "      1200     2200       0.0   0.0     0.0   0.0\n",
       "      1250     2250       0.0   0.0     0.0   0.0\n",
       "      1300     2300       0.0   0.0     0.0   0.0\n",
       "      1350     2350       0.0   0.0     0.0   0.0\n",
       "      1400     2400       0.0   0.0     0.0   0.0\n",
       "      1450     2450       0.0   0.0     0.0   0.0\n",
       "...                       ...   ...     ...   ...\n",
       "chr19 59126500 59127500   0.0   0.0     0.0   0.0\n",
       "      59126550 59127550   0.0   0.0     0.0   0.0\n",
       "      59126600 59127600   0.0   0.0     0.0   0.0\n",
       "      59126650 59127650   0.0   0.0     0.0   0.0\n",
       "      59126700 59127700   0.0   0.0     0.0   0.0\n",
       "      59126750 59127750   0.0   0.0     0.0   0.0\n",
       "      59126800 59127800   0.0   0.0     0.0   0.0\n",
       "      59126850 59127850   0.0   0.0     0.0   0.0\n",
       "      59126900 59127900   0.0   0.0     0.0   0.0\n",
       "      59126950 59127950   0.0   0.0     0.0   0.0\n",
       "      59127000 59128000   0.0   0.0     0.0   0.0\n",
       "      59127050 59128050   0.0   0.0     0.0   0.0\n",
       "      59127100 59128100   0.0   0.0     0.0   0.0\n",
       "      59127150 59128150   0.0   0.0     0.0   0.0\n",
       "      59127200 59128200   0.0   0.0     0.0   0.0\n",
       "      59127250 59128250   0.0   0.0     0.0   0.0\n",
       "      59127300 59128300   0.0   0.0     0.0   0.0\n",
       "      59127350 59128350   0.0   0.0     0.0   0.0\n",
       "      59127400 59128400   0.0   0.0     0.0   0.0\n",
       "      59127450 59128450   0.0   0.0     0.0   0.0\n",
       "      59127500 59128500   0.0   0.0     0.0   0.0\n",
       "      59127550 59128550   0.0   0.0     0.0   0.0\n",
       "      59127600 59128600   0.0   0.0     0.0   0.0\n",
       "      59127650 59128650   0.0   0.0     0.0   0.0\n",
       "      59127700 59128700   0.0   0.0     0.0   0.0\n",
       "      59127750 59128750   0.0   0.0     0.0   0.0\n",
       "      59127800 59128800   0.0   0.0     0.0   0.0\n",
       "      59127850 59128850   0.0   0.0     0.0   0.0\n",
       "      59127900 59128900   0.0   0.0     0.0   0.0\n",
       "      59127950 59128950   0.0   0.0     0.0   0.0\n",
       "\n",
       "[6046528 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We load our test set labels into memory here, as we will use them to measure performance in cases 1 - 4 below.\n",
    "#Note that we only load the labels into memory, not the actual test dataset. \n",
    "\n",
    "#It is not necessary to load the training/validation dataset or labels, see below. \n",
    "test_set=pd.read_hdf(\"TF.test.hdf5\").set_index(['CHR','START','END'])\n",
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges of in vivo data : batch generators and class upsampling <a name='3'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "In tutorials 1 - 3, we used the [keras fit](https://keras.io/models/sequential/#fit) function to train a CNN. However, when working with real data we face two new challenges: \n",
    "\n",
    "1) The dataset is much bigger. In our training set, there are 50,881,560 1kb bins, in our validation set, there are 4,984,994 bins, and in our test set there are 6,046,529 bins. Loading this dataset into memory to pass as a numpy array to the CNN code will require more memory than is available on many machines. Consequently, we use the [keras fit_generator](https://keras.io/models/sequential/#fit_generator) function to limit the memory footprint. This function reads in one batch of training and one batch of validation data at a time from a python generator. in *dragonn.generators*, we provide several python generator functions to match the scenarios below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.generators import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The dataset is highly imbalanced. Of the 50,881,560 1kb bins in the training set, only \n",
    "\n",
    "* 136,279 are labeled positive for the SPI1 task (372 negatives: 1 positive )\n",
    "\n",
    "* 131,245 are labeled positive for the CTCF task (387 negatives: 1 positive ) \n",
    "\n",
    "* 93,981 are labeled positive for the ZNF143 task (540 negatives: 1 positive ) \n",
    "\n",
    "* 15,641 are labeled positive for the SIX5 task (3252 negatives: 1 positive ) \n",
    "\n",
    "The class imbalance is far too high for the model to learn unassisted. Hence, we upsample the positive bins to include in each batch with the \"upsample\" argument to data_generator. The upsample argument accepts a fraction between 0 and 1 and ensures that this fraction of the batch consists of positive bins. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To prepare for model training, we import the necessary functions and submodules from keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta, SGD, RMSprop;\n",
    "import keras.losses;\n",
    "from keras.constraints import maxnorm;\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping, History\n",
    "from keras import backend as K \n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concise.metrics import tpr, tnr, fpr, fnr, precision, f1\n",
    "def initialize_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=(1,1000,4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(1,35)))\n",
    "\n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=15,kernel_size=(1,10)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(ntasks))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',\n",
    "                  metrics=[tpr,\n",
    "                           tnr,\n",
    "                           fpr,\n",
    "                           fnr,\n",
    "                           precision,\n",
    "                           f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Negatives consist of shuffled references, single-tasked models<a name='4'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by training a model on the SPI1 and CTCF dataset with the following specifications: \n",
    "\n",
    "* We use dinucleotide-shuffled positive bins as the negative set. \n",
    "\n",
    "* Each batch contains one-hot encoded 1kb regions from the genome, as well as the one-hot-encoded reverse complement sequences of those regions. \n",
    "\n",
    "* We ensure that at least 10% of the samples in each batch are positives \n",
    "\n",
    "We create generators for the training and validation data to meet these specifications: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators\n",
    "from dragonn.generators import * \n",
    "case1_spi1_train_gen=DataGenerator(\"SPI1.positives.TF.train.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False)\n",
    "case1_spi1_valid_gen=DataGenerator(\"SPI1.positives.TF.valid.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False)\n",
    "case1_ctcf_train_gen=DataGenerator(\"CTCF.positives.TF.train.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False)\n",
    "case1_ctcf_valid_gen=DataGenerator(\"CTCF.positives.TF.valid.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now follow the standard protocol we used in tutorials 1 - 3 to train a keras model, with the exception that we use the fit_generator function in keras, rather than the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      " 997/1000 [============================>.] - ETA: 0s - loss: 0.2796 - sensitivity: 0.8548 - specificity: 0.8933 - fpr: 0.1067 - fnr: 0.1452 - precision: 0.8977 - f1: 0.8654\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 0.2791 - sensitivity: 0.8550 - specificity: 0.8935 - fpr: 0.1065 - fnr: 0.1450 - precision: 0.8980 - f1: 0.8656 - val_loss: 0.1229 - val_sensitivity: 0.9435 - val_specificity: 0.9683 - val_fpr: 0.0317 - val_fnr: 0.0565 - val_precision: 0.9682 - val_f1: 0.9543\n",
      "Epoch 2/150\n",
      "1000/1000 [==============================] - 28s 28ms/step - loss: 0.1343 - sensitivity: 0.9374 - specificity: 0.9587 - fpr: 0.0413 - fnr: 0.0626 - precision: 0.9612 - f1: 0.9460 - val_loss: 0.1333 - val_sensitivity: 0.8964 - val_specificity: 0.9976 - val_fpr: 0.0024 - val_fnr: 0.1036 - val_precision: 0.9974 - val_f1: 0.9435\n",
      "Epoch 3/150\n",
      "1000/1000 [==============================] - 28s 28ms/step - loss: 0.0965 - sensitivity: 0.9580 - specificity: 0.9681 - fpr: 0.0319 - fnr: 0.0420 - precision: 0.9697 - f1: 0.9620 - val_loss: 0.1077 - val_sensitivity: 0.9120 - val_specificity: 0.9981 - val_fpr: 0.0019 - val_fnr: 0.0880 - val_precision: 0.9980 - val_f1: 0.9524\n",
      "Epoch 4/150\n",
      "1000/1000 [==============================] - 28s 28ms/step - loss: 0.0850 - sensitivity: 0.9649 - specificity: 0.9724 - fpr: 0.0276 - fnr: 0.0351 - precision: 0.9739 - f1: 0.9679 - val_loss: 0.0807 - val_sensitivity: 0.9381 - val_specificity: 0.9973 - val_fpr: 0.0027 - val_fnr: 0.0619 - val_precision: 0.9972 - val_f1: 0.9663\n",
      "Epoch 5/150\n",
      " 257/1000 [======>.......................] - ETA: 10s - loss: 0.0734 - sensitivity: 0.9705 - specificity: 0.9765 - fpr: 0.0235 - fnr: 0.0295 - precision: 0.9779 - f1: 0.9730\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0622 - sensitivity: 0.9746 - specificity: 0.9797 - fpr: 0.0203 - fnr: 0.0254 - precision: 0.9803 - f1: 0.9770 - val_loss: 0.0449 - val_sensitivity: 0.9759 - val_specificity: 0.9926 - val_fpr: 0.0074 - val_fnr: 0.0241 - val_precision: 0.9926 - val_f1: 0.9840\n",
      "Epoch 6/150\n",
      "1000/1000 [==============================] - 28s 28ms/step - loss: 0.0543 - sensitivity: 0.9784 - specificity: 0.9818 - fpr: 0.0182 - fnr: 0.0216 - precision: 0.9822 - f1: 0.9800 - val_loss: 0.0878 - val_sensitivity: 0.9327 - val_specificity: 0.9988 - val_fpr: 0.0012 - val_fnr: 0.0673 - val_precision: 0.9988 - val_f1: 0.9642\n",
      "Epoch 7/150\n",
      "1000/1000 [==============================] - 26s 26ms/step - loss: 0.0472 - sensitivity: 0.9810 - specificity: 0.9845 - fpr: 0.0155 - fnr: 0.0190 - precision: 0.9848 - f1: 0.9827 - val_loss: 0.0335 - val_sensitivity: 0.9829 - val_specificity: 0.9940 - val_fpr: 0.0060 - val_fnr: 0.0171 - val_precision: 0.9940 - val_f1: 0.9883\n",
      "Epoch 8/150\n",
      "1000/1000 [==============================] - 29s 29ms/step - loss: 0.0457 - sensitivity: 0.9826 - specificity: 0.9848 - fpr: 0.0152 - fnr: 0.0174 - precision: 0.9852 - f1: 0.9837 - val_loss: 0.0450 - val_sensitivity: 0.9688 - val_specificity: 0.9990 - val_fpr: 0.0010 - val_fnr: 0.0312 - val_precision: 0.9990 - val_f1: 0.9835\n",
      "Epoch 9/150\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.0402 - sensitivity: 0.9846 - specificity: 0.9863 - fpr: 0.0138 - fnr: 0.0154 - precision: 0.9866 - f1: 0.9854 - val_loss: 0.0295 - val_sensitivity: 0.9840 - val_specificity: 0.9967 - val_fpr: 0.0033 - val_fnr: 0.0160 - val_precision: 0.9967 - val_f1: 0.9902\n",
      "Epoch 10/150\n",
      "1000/1000 [==============================] - 29s 29ms/step - loss: 0.0365 - sensitivity: 0.9858 - specificity: 0.9886 - fpr: 0.0114 - fnr: 0.0142 - precision: 0.9888 - f1: 0.9871 - val_loss: 0.0372 - val_sensitivity: 0.9751 - val_specificity: 0.9986 - val_fpr: 0.0014 - val_fnr: 0.0249 - val_precision: 0.9986 - val_f1: 0.9866\n",
      "Epoch 11/150\n",
      "1000/1000 [==============================] - 27s 27ms/step - loss: 0.0360 - sensitivity: 0.9860 - specificity: 0.9878 - fpr: 0.0122 - fnr: 0.0140 - precision: 0.9881 - f1: 0.9869 - val_loss: 0.0252 - val_sensitivity: 0.9857 - val_specificity: 0.9978 - val_fpr: 0.0022 - val_fnr: 0.0143 - val_precision: 0.9978 - val_f1: 0.9916\n",
      "Epoch 12/150\n",
      "1000/1000 [==============================] - 28s 28ms/step - loss: 0.0334 - sensitivity: 0.9875 - specificity: 0.9887 - fpr: 0.0113 - fnr: 0.0125 - precision: 0.9890 - f1: 0.9881 - val_loss: 0.0304 - val_sensitivity: 0.9797 - val_specificity: 0.9989 - val_fpr: 0.0011 - val_fnr: 0.0203 - val_precision: 0.9989 - val_f1: 0.9891\n",
      "Epoch 13/150\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.0322 - sensitivity: 0.9879 - specificity: 0.9896 - fpr: 0.0104 - fnr: 0.0121 - precision: 0.9898 - f1: 0.9887 - val_loss: 0.0203 - val_sensitivity: 0.9934 - val_specificity: 0.9948 - val_fpr: 0.0053 - val_fnr: 0.0066 - val_precision: 0.9948 - val_f1: 0.9941\n",
      "Epoch 14/150\n",
      "1000/1000 [==============================] - 28s 28ms/step - loss: 0.0295 - sensitivity: 0.9889 - specificity: 0.9903 - fpr: 0.0097 - fnr: 0.0111 - precision: 0.9905 - f1: 0.9896 - val_loss: 0.0290 - val_sensitivity: 0.9808 - val_specificity: 0.9991 - val_fpr: 9.0625e-04 - val_fnr: 0.0192 - val_precision: 0.9991 - val_f1: 0.9898\n",
      "Epoch 15/150\n",
      "1000/1000 [==============================] - 29s 29ms/step - loss: 0.0289 - sensitivity: 0.9892 - specificity: 0.9908 - fpr: 0.0092 - fnr: 0.0108 - precision: 0.9910 - f1: 0.9899 - val_loss: 0.0173 - val_sensitivity: 0.9923 - val_specificity: 0.9973 - val_fpr: 0.0027 - val_fnr: 0.0077 - val_precision: 0.9973 - val_f1: 0.9947\n",
      "Epoch 16/150\n",
      "1000/1000 [==============================] - 28s 28ms/step - loss: 0.0265 - sensitivity: 0.9897 - specificity: 0.9915 - fpr: 0.0085 - fnr: 0.0103 - precision: 0.9917 - f1: 0.9906 - val_loss: 0.0214 - val_sensitivity: 0.9870 - val_specificity: 0.9986 - val_fpr: 0.0014 - val_fnr: 0.0130 - val_precision: 0.9986 - val_f1: 0.9927\n",
      "Epoch 17/150\n",
      " 738/1000 [=====================>........] - ETA: 3s - loss: 0.0287 - sensitivity: 0.9889 - specificity: 0.9909 - fpr: 0.0091 - fnr: 0.0111 - precision: 0.9912 - f1: 0.9899"
     ]
    }
   ],
   "source": [
    "#Train the SPI1 model \n",
    "case1_spi1_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case1_spi1=case1_spi1_model.fit_generator(case1_spi1_train_gen,\n",
    "                                                  validation_data=case1_spi1_valid_gen,\n",
    "                                                  steps_per_epoch=1000,\n",
    "                                                  validation_steps=1000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the CTCF model \n",
    "case1_ctcf_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case1_ctcf=case1_ctcf_model.fit_generator(case1_ctcf_train_gen,\n",
    "                                                  validation_data=case1_ctcf_valid_gen,\n",
    "                                                  steps_per_epoch=1000,\n",
    "                                                  validation_steps=1000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "from dragonn.tutorial_utils import plot_learning_curve\n",
    "plot_learning_curve(history_case1_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curve for CTCF \n",
    "plot_learning_curve(history_case1_ctcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now measure how well the models performed by calculating performance metrics on the test splits across the whole genome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case1_spi1_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['SPI1'])\n",
    "case1_spi1_test_predictions=case1_spi1_model.predict_generator(case1_spi1_test_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case1_ctcf_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['CTCF'])\n",
    "case1_ctcf_test_predictions=case1_ctcf_model.predict_generator(case1_ctcf_test_gen,max_queue_size=5000, workers=40, use_multiprocessing=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format true & predicted test labels for performance assessment \n",
    "\n",
    "#if test_set.shape is not a multiple of batch_size, \n",
    "#there may be some extra values in test_set that need to get truncated.\n",
    "spi1_test_truth=np.expand_dims(test_set['SPI1'][0:case1_spi1_test_predictions.shape[0]],1).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "from dragonn.metrics import ClassificationResult\n",
    "print(ClassificationResult(spi1_test_truth,case1_spi1_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctcf_test_truth=np.expand_dims(test_set['CTCF'][0:case1_ctcf_test_predictions.shape[0]],1).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(ctcf_test_truth,case1_ctcf_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Whole-genome negatives, single-tasked models <a name='5'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators\n",
    "from dragonn.generators import * \n",
    "case2_spi1_train_gen=DataGenerator(\"TF.train.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"SPI1\"],upsample_ratio=0.1)\n",
    "case2_spi1_valid_gen=DataGenerator(\"TF.valid.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"SPI1\"],upsample_ratio=0.1)\n",
    "case2_ctcf_train_gen=DataGenerator(\"TF.train.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"CTCF\"],upsample_ratio=0.1)\n",
    "case2_ctcf_valid_gen=DataGenerator(\"TF.valid.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"CTCF\"],upsample_ratio=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 model \n",
    "case2_spi1_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case2_spi1=case2_spi1_model.fit_generator(case2_spi1_train_gen,\n",
    "                                                  validation_data=case2_spi1_valid_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=10000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the CTCF model \n",
    "case2_ctcf_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case2_ctcf=case2_ctcf_model.fit_generator(case2_ctcf_train_gen,\n",
    "                                                  validation_data=case2_ctcf_valid_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=10000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "plot_learning_curve(history_case2_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for CTCF  \n",
    "plot_learning_curve(history_case2_ctcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get model predictions on the test set \n",
    "case2_spi1_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['SPI1'])\n",
    "case2_spi1_test_predictions=case2_spi1_model.predict_generator(case2_spi1_test_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case2_ctcf_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['CTCF'])\n",
    "case2_ctcf_test_predictions=case2_ctcf_model.predict_generator(case2_ctcf_test_gen,max_queue_size=5000, workers=40, use_multiprocessing=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.metrics import ClassificationResult\n",
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(spi1_test_truth,case2_spi1_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(ctcf_test_truth,case2_ctcf_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Whole-genome negatives, multi-tasked models <a name='6'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators for multi-tasked models. Guarantee 10% positives in each batch \n",
    "case3_train_gen=DataGenerator(\"TF.train.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.1)\n",
    "case3_valid_gen=DataGenerator(\"TF.valid.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 model \n",
    "case3_model=initialize_model(4)\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case3=case3_model.fit_generator(case3_train_gen,\n",
    "                                        validation_data=case3_valid_gen,\n",
    "                                        steps_per_epoch=1000,\n",
    "                                        validation_steps=1000,\n",
    "                                        epochs=10,\n",
    "                                        verbose=1,\n",
    "                                        multiprocessing=True,\n",
    "                                        workers=40,\n",
    "                                        max_queue_size=100,\n",
    "                                        callbacks=[EarlyStopping(patience=3),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for the multi-tasked model   \n",
    "plot_learning_curve(history_case3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case3_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                             \"hg19.genome.fa.gz\",\n",
    "                             upsample=False,\n",
    "                             add_revcomp=False,\n",
    "                             batch_size=1000,\n",
    "                             tasks=['SPI1'])\n",
    "case3_test_predictions=case3_model.predict_generator(case3_test_gen,\n",
    "                                                     max_queue_size=5000, \n",
    "                                                     workers=40, \n",
    "                                                     use_multiprocessing=True, \n",
    "                                                     verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_truth=test_set[0:case3_test_predictions.shape[0]].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(test_truth,case3_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: What happens if we don't upsample positive examples in our batches? <a name='7'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators\n",
    "case4_spi1_train_gen=DataGenerator(\"TF.train.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"SPI1\"],upsample=False)\n",
    "case4_spi1_valid_gen=DataGenerator(\"TF.valid.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"SPI1\"],upsample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 model \n",
    "case4_spi1_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case4_spi1=case4_spi1_model.fit_generator(case4_spi1_train_gen,\n",
    "                                                  validation_data=case4_spi1_valid_gen,\n",
    "                                                  steps_per_epoch=1000,\n",
    "                                                  validation_steps=1000,\n",
    "                                                  epochs=10,\n",
    "                                                  verbose=1,\n",
    "                                                  multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "plot_learning_curve(history_case4_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We  use a custom batch_predict function to generate predictions on the test set, one batch at a time \n",
    "case4_spi1_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['SPI1'])\n",
    "case4_spi1_test_predictions=case4_spi1_model.predict_generator(case4_spi1_test_gen,max_queue_size=5000, workers=40, use_multiprocessing=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(spi1_test_truth,case4_spi1_test_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT <a name='8'>\n",
    "<a href=#outline>Home</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a name='9'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
